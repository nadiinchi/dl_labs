{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 3. Training convolutional networks in Pytroch\n",
    "\n",
    "On this seminar, we will train Lenet-5 on a MNIST dataset using pytorch.\n",
    "\n",
    "For the beginning, please read several examples of training CNNs in pytroch:\n",
    "* [Example 1](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py)\n",
    "* [Example 2](https://github.com/jcjohnson/pytorch-examples/blob/master/nn/two_layer_net_nn.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will works with a MNIST dataset. It contains grayscale images of handwritten digits of size 28 x 28. The number of training objects is 60000. \n",
    "\n",
    "\n",
    "In pytorch, there is a special module to download MNIST. But for us it is more convinient to load the data ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below prepares short data (train and val) for seminar purposes (use this data to quickly learn model on CPU and to tune the hyperparameters). Also, we prepare the full data (train_full and test) to train a final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "np.random.seed(0)\n",
    "idxs = np.random.permutation(np.arange(X_train.shape[0]))\n",
    "X_train, y_train = X_train[idxs], y_train[idxs]\n",
    "                            \n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch offers convinient class DataLoader for mini batch generation. You should pass instance of Tensor Dataset to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loader(X, y, batch_size=64):\n",
    "    train = torch.utils.data.TensorDataset(torch.from_numpy(X).float(), \n",
    "                                       torch.from_numpy(y))\n",
    "    train_loader = torch.utils.data.DataLoader(train, \n",
    "                                               batch_size=batch_size)\n",
    "    return train_loader\n",
    "\n",
    "# for final model:\n",
    "train_loader_full = get_loader(X_train, y_train) \n",
    "test_loader = get_loader(X_test, y_test)\n",
    "# for validation purposes:\n",
    "train_loader = get_loader(X_train[:15000], y_train[:15000])\n",
    "val_loader = get_loader(X_train[15000:30000], y_train[15000:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check number of objects\n",
    "val_loader.dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building LeNet-5\n",
    "\n",
    "Convolutional layer (from Anton Osokin's presentation):\n",
    "![slide](https://github.com/nadiinchi/dl_labs/raw/master/convolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement Lenet-5:\n",
    "\n",
    "![Архитектура LeNet-5](https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/lenet-5-825x285.png?x64257)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a network according to the image and code examples given above. Use ReLU nonlinearity (after all linear and convolutional layers). The network must support multiplying the number of convolutions in each convolutional layer by k.\n",
    "\n",
    "Please note that on the scheme the size of the image is 32 x 32 but in our code the isze is 28 x 28.\n",
    "\n",
    "Do not apply softmax at the end of the forward pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, k=1):\n",
    "        super(CNN, self).__init__()\n",
    "        ### your code here: define layers\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        ### your code here: transform x using layers\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of the parameters in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(param.data.numpy().size for param \\\n",
    "               in model.parameters() if param.requires_grad)\n",
    "\n",
    "count_parameters(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # loss includes softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, define a device where to store the data and the model (cpu or gpu):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "cnn = cnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we will control the quality on the training and validation set. This produces duplicates of the code. That's why we will define a function evaluate_loss_acc to evaluate our model on different data sets. In the same manner, we define function train_epoch to perform one training epoch on traiing data. Please note that we will compute the training loss _after_ each epoch (not averaging it during epoch).\n",
    "\n",
    "In the propotypes, train and eval modes are noted. In our case, we don't need them (because we don't use neither dropout nor batch normalization). However, we will switch the regime so you can use this code in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    for each batch \n",
    "    performs forward and backward pass and parameters update \n",
    "    \n",
    "    Input:\n",
    "    model: instance of model (example defined above)\n",
    "    optimizer: instance of optimizer (defined above)\n",
    "    train_loader: instance of DataLoader\n",
    "    \n",
    "    Returns:\n",
    "    nothing\n",
    "    \n",
    "    Do not forget to set net to train mode!\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    \n",
    "\n",
    "def evaluate_loss_acc(loader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates loss and accuracy on the whole dataset\n",
    "    \n",
    "    Input:\n",
    "    loader:  instance of DataLoader\n",
    "    model: instance of model (examle defined above)\n",
    "    \n",
    "    Returns:\n",
    "    (loss, accuracy)\n",
    "    \n",
    "    Do not forget to set net to eval mode!\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    \n",
    "    \n",
    "def train(model, opt, train_loader, test_loader, criterion, n_epochs, \\\n",
    "          device, verbose=True):\n",
    "    \"\"\"\n",
    "    Performs training of the model and prints progress\n",
    "    \n",
    "    Input:\n",
    "    model: instance of model (example defined above)\n",
    "    opt: instance of optimizer \n",
    "    train_loader: instance of DataLoader\n",
    "    test_loader: instance of DataLoader (for evaluation)\n",
    "    n_epochs: int\n",
    "    \n",
    "    Returns:\n",
    "    4 lists: train_log, train_acc_log, val_log, val_acc_log\n",
    "    with corresponding metrics per epoch\n",
    "    \"\"\"\n",
    "    train_log, train_acc_log = [], []\n",
    "    val_log, val_acc_log = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_epoch(model, opt, train_loader, criterion, device)\n",
    "        train_loss, train_acc = evaluate_loss_acc(train_loader, \n",
    "                                                  model, criterion, \n",
    "                                                  device)\n",
    "        val_loss, val_acc = evaluate_loss_acc(test_loader, model, \n",
    "                                              criterion, device)\n",
    "\n",
    "        train_log.append(train_loss)\n",
    "        train_acc_log.append(train_acc)\n",
    "\n",
    "        val_log.append(val_loss)\n",
    "        val_acc_log.append(val_acc)\n",
    "        \n",
    "        if verbose:\n",
    "             print (('Epoch [%d/%d], Loss (train/test): %.4f/%.4f,'+\\\n",
    "               ' Acc (train/test): %.4f/%.4f' )\n",
    "                   %(epoch+1, n_epochs, \\\n",
    "                     train_loss, val_loss, train_acc, val_acc))\n",
    "            \n",
    "    return train_log, train_acc_log, val_log, val_acc_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network, using defined functions. Use Adam as an optimizer, learning_rate=0.001, number of epochs = 20. For hold out, use val_loader, not test_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the activations of the network when images pass through it. The code below draws a table of images: the first column shows original images, the following 6 columns show images when the filters are applied to them. To use this code, save batch containing 10 images to x and activations of first layer on these images to y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(x, y):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for im in range(10):\n",
    "        plt.subplot(11, 7, im*7+1)\n",
    "        plt.imshow(x.data[im, 0])\n",
    "        plt.axis(\"off\")\n",
    "        for i in range(6):\n",
    "            plt.subplot(11, 7, im*7+i+2)\n",
    "            plt.imshow(y.data[im, i].numpy())\n",
    "            plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize the result of applying the second layer to images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Choosing  learning_rate and batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot accuracy on the training and testing set v. s. training epoch for different learning parameters: learning rate$ \\in \\{0.0001, 0.001, 0.01\\}$, batch size $\\in \\{64, 256\\}$. \n",
    "\n",
    "The best option is to plot training curves on the left graph and validation curves on the right graph with the shared y axis (use plt.ylim).\n",
    "\n",
    "How do learnign rate and batch size affect the final quality of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to modify our architecture: to raise the number of filters and to reduce the number of fully-connected layers.\n",
    "\n",
    "Insert numbers in the brackets:\n",
    "* LeNet-5 classic (6 and 16 convolutions):  training acc: ( )  validation acc: ( )\n",
    "* Number of convolutions x 4 (24 и 64 convolutions):  training acc: ( )  validation acc: ( )\n",
    "* Removing fully connected layer: the previous network with 1 FC layer: training acc: ( )  validation acc: ( )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the learning rate, batch size and the architecture based on your experiments. Train a network on the full dataset and print accuracy on the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
