{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генеративно-состязательные сети (Generative Adversarial Networks)\n",
    "\n",
    "В задании предлагается реализовать GAN, обучить её на MNIST, оценить правдоподобие и сделать выводы.\n",
    "\n",
    "Необходимая теория приведена ниже.\n",
    "\n",
    "Актуальная версия доступна по адресу https://github.com/nadiinchi/dl_labs/blob/master/lab_gan.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "Дана выборка независимых одинаково распределенных величин из истинного распределения $x_i \\sim p_d(x)$, $i = 1, \\dots, N$.\n",
    "\n",
    "Задача - построить вероятностную модель $p_\\theta(x)$ истинного распределения $p_d(x)$.\n",
    "\n",
    "Распределение $p_\\theta(x)$ должно позволять как оценить плотность вероятности для данного объекта $x$, так и сэмплировать $x \\sim p_\\theta(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вероятностная модель\n",
    "$z \\in \\mathbb{R}^d$ - локальная латентная переменная, т. е. своя для каждого объекта $x \\in \\mathbb{R}^D$.\n",
    "\n",
    "Генеративный процесс вариационного автокодировщика:\n",
    "1. Сэмплируем $z \\sim p(z)$.\n",
    "2. $x = G_\\theta(z)$.\n",
    "\n",
    "Параметры преобразования $G_\\theta(z)$ задаются нейросетью с весами $\\theta$, получающей на вход вектор $z$.\n",
    "\n",
    "Индуцированная генеративным процессом плотность вероятности объекта $x$:\n",
    "\n",
    "$$p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} \\delta(x = G(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка логарифма правдоподобия\n",
    "Для оценки логарифма правдоподобия используется метод Парзеновского окна/ядерного сглаживания (англ. Kernel Density Estimation/Parzen–Rosenblatt window method). Концептуально он заключается в том, что мы сглаживаем модельное распределение, и по этому сглаженному распределению вычисляем правдоподобие модели.\n",
    "\n",
    "$$p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} \\delta(x = G(z)) \\approx \\frac{1}{Mh^D}\\sum_{i=1}^M K\\left(\\frac{x - G_\\theta(z_i)}{h}\\right)$$\n",
    "\n",
    "Здесь $K(x)$ - любое распределение, а $h$ - ширина окна. Тогда выполняется\n",
    "\n",
    "$$\\mathbb{E}_{x \\sim p_d} \\log p_\\theta(x) \\approx  \\frac{1}{N}\\sum_{i=1}^{N} \\log \\frac{1}{Mh^D}\\sum_{j=1}^M K\\left(\\frac{x_i - G_\\theta(z_j)}{h}\\right)$$\n",
    "\n",
    "В генеративно-состязательных сетях для оценки правдоподобия используется стандартное нормальное распределение $K(x) = N(x | 0, I)$. Тогда получаем\n",
    "\n",
    "$$\\mathbb{E}_{x \\sim p_d} \\log p_\\theta(x) \\approx  \\frac{1}{N}\\sum_{i=1}^{N} \\log \\frac{1}{M}\\sum_{j=1}^M \\prod_{k=1}^D\\frac{1}{\\sqrt{2 \\pi} \\sigma}\\exp\\left(-\\frac{(x_{i,k} - G(z_j)_k)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "Коэффициент $\\sigma$ настраивается на валидационной выборке и с его помощью считается правдоподобие тестовой выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка, нормировка и визуалиация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNIST('mnist', download=True, train=True)\n",
    "train_data = TensorDataset(data.train_data.view(-1, 28 * 28).float() / 255)\n",
    "data = MNIST('mnist', download=True, train=False)\n",
    "test_data_raw = TensorDataset(data.test_data.view(-1, 28 * 28).float() / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_size = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.tensors = (nn.AvgPool2d(2, 2)(train_data.tensors[0].view(-1, 28, 28)).data.view(-1, 196), )\n",
    "test_data_raw.tensors = (nn.AvgPool2d(2, 2)(test_data_raw.tensors[0].view(-1, 28, 28)).data.view(-1, 196), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = TensorDataset(test_data_raw.tensors[0][:5000])\n",
    "test_data = TensorDataset(test_data_raw.tensors[0][5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_images(x):\n",
    "    plt.figure(figsize=(12, 12 / 10 * (x.shape[0] // 10 + 1)))\n",
    "    x = x.view(-1, digit_size, digit_size)\n",
    "    for i in range(x.shape[0]):\n",
    "        plt.subplot(x.shape[0] // 10 + 1, 10, i + 1)\n",
    "        plt.imshow(x.data[i].numpy(), cmap='Greys_r', vmin=0, vmax=1, interpolation='lanczos')\n",
    "        plt.axis('off')\n",
    "\n",
    "show_images(train_data[:10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "def draw_manifold(generator):\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "\n",
    "            x_decoded = generator(z_sample)\n",
    "            digit = x_decoded\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap='Greys_r', vmin=0, vmax=1, interpolation='lanczos')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции и классы, описывающие модель и процесс её обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Запоминает размерности, в которые при проходе\n",
    "        вперед будут переводиться все объекты.\n",
    "        Например,\n",
    "            input = torch.zeros(100, 196)\n",
    "            reshape_layer = Reshape(1, 14, 14)\n",
    "            reshape_layer(input)\n",
    "        возвращает тензор размерности (100, 1, 14, 14).\n",
    "            input = torch.zeros(100, 1, 14, 14)\n",
    "            reshape_layer = Reshape(-1)\n",
    "            reshape_layer(input)\n",
    "        наоборот вернет тензор размерности (100, 196).\n",
    "        \"\"\"\n",
    "        super(type(self), self).__init__()\n",
    "        self.dims = args\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Возвращает тензор с измененными размерностями объектов.\n",
    "        Вход: input, FloatTensor.\n",
    "        Возвращаемое значение: FloatTensor.\n",
    "        \"\"\"\n",
    "        return input.view(input.size(0), *self.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        \"\"\"\n",
    "        Инициализирует веса модели.\n",
    "        Вход: d, int - размерность латентного пространства.\n",
    "        Вход: D, int - размерность пространства объектов.\n",
    "        \"\"\"\n",
    "        super(type(self), self).__init__()\n",
    "        self.d = d\n",
    "        # Можно пробовать другие архитектуры: как более сложные\n",
    "        # сверточные, так и более простые, например, полносвязные.\n",
    "        # Однако желательно обучить хотя бы одну сверточную модель.\n",
    "        self.discriminator = nn.Sequential(\n",
    "            Reshape(1, digit_size, digit_size),\n",
    "            nn.Conv2d(1, 64, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 32, 3, 2, 0, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 16, 3, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            Reshape(-1),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.generator = nn.Sequential(\n",
    "            Reshape(self.d, 1, 1),\n",
    "            nn.ConvTranspose2d(self.d, 128, 4, 1, 0, 0, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, 2, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, 3, 2, 1, 1, bias=False),\n",
    "            Reshape(-1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def generate_noise(self, num_samples):\n",
    "        \"\"\"\n",
    "        Генерирует сэмплы из априорного распределения на z.\n",
    "        Вход: num_samples, int - число сэмплов, которые надо сгененрировать.\n",
    "        Возвращаемое значение: Tensor, матрица размера num_samples x d.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "        if next(self.parameters()).is_cuda:\n",
    "            z = z.cuda()\n",
    "        return z\n",
    "\n",
    "    def generate_samples(self, num_samples):\n",
    "        \"\"\"\n",
    "        Генерирует сэмплы из индуцируемого моделью распределения на объекты x.\n",
    "        Вход: num_samples, int - число сэмплов, которые надо сгененрировать.\n",
    "        Возвращаемое значение: Tensor, матрица размера num_samples x D.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def discriminator_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Вычисляет значение функции потерь дискриминатора на данном батче.\n",
    "        Возвращаемая оценка должна быть дифференцируема по параметрам модели (!).\n",
    "        Вход: batch, FloatTensor - матрица объектоа размера n x D.\n",
    "        Возвращаемое значение: Tensor, скаляр - значение функции потерь\n",
    "        дискриминатора на данном батче.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def generator_loss(self, batch_size):\n",
    "        \"\"\"\n",
    "        Вычисляет значение функции потерь генератора на данном батче.\n",
    "        Возвращаемая оценка должна быть дифференцируема по параметрам модели (!).\n",
    "        Вход: batch, FloatTensor - матрица объектоа размера n x D.\n",
    "        Возвращаемое значение: Tensor, скаляр - значение функции потерь\n",
    "        генератора на данном батче.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mean_exp(mtx):\n",
    "    \"\"\"\n",
    "    Возвращает логарифм среднего по каждому столбцу от экспоненты данной матрицы.\n",
    "    Подсказка: не забывайте про вычислительную стабильность!\n",
    "    Вход: mtx, Tensor - матрица размера n x k.\n",
    "    Возвращаемое значение: Tensor, вектор длины n.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(generated_set, validation_set, test_set):\n",
    "    \"\"\"\n",
    "    Возвращает оценку логарифма правдоподобия модели GAN методом\n",
    "    Парзеновского окна со стандартным нормальным ядром.\n",
    "    Подсказка: sigma должна настраиваться по валидационной выборке, а\n",
    "    правдоподобие считаться по тестовой.\n",
    "    Подсказка: вместо sigma можно настраивать log_sigma.\n",
    "    Подсказка: для настойки sigma допустимо использовать как перебор по сетке,\n",
    "    так и другие методы опимизации.\n",
    "    Вход: generated_set - сэмплы из генеративной модели.\n",
    "    Вход: validation_set - валидационная выборка.\n",
    "    Вход: test_set - тестовая выборка.\n",
    "    Возвращаемое значение: float (не Tensor!) - оценка логарифма правдоподобия.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_many_samples(model, num_samples, batch_size):\n",
    "    size = 0\n",
    "    res = []\n",
    "    while size < num_samples:\n",
    "        res.append(model.generate_samples(min(batch_size, num_samples - size)))\n",
    "        size += batch_size\n",
    "    return torch.cat(res, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Обучает модель.\n",
    "    Вход: model, Module - объект, модель.\n",
    "    У этого объекта должна быть функция batch_loss от batch - FloatTensor и K - int,\n",
    "    возвращающая скаляр Variable - функцию потерь на батче, которая должна быть\n",
    "    оптимизирована.\n",
    "    Вход: k, int - число итераций оптимизации дискриминатора на итерацию оптимизации\n",
    "    генератора.\n",
    "    Вход: batch_size, int.\n",
    "    Вход: num_epochs, int.\n",
    "    Вход: learning_rate, float.\n",
    "    Возвращаемое значение: словарь с полями 'model' - обученная модель,\n",
    "    'generator_losses' - список значений функции потерь генератора,\n",
    "    'discriminator_losses' - список значений функции потерь дискриминатора.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    # возможно, нужно подобрать другие параметры, чтобы модель обучилась\n",
    "    batch_size = 64\n",
    "    learning_rate = 2e-4\n",
    "\n",
    "    # возможно, нужно использовать другие методы оптимизации или параметры методов оптимизации,\n",
    "    # чтобы модель обучилась\n",
    "    gd_generator = optim.Adam(model.generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    gd_discriminator = optim.RMSprop(model.discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    generator_losses = [0]\n",
    "    discriminator_losses = [0]\n",
    "    log_likelihoods = []\n",
    "\n",
    "    generated_samples = generate_many_samples(model, 512, batch_size).detach()\n",
    "    valid_samples = valid_data[np.random.choice(len(valid_data), 512, False)][0]\n",
    "    valid_samples = valid_samples.to(next(model.parameters()).device)\n",
    "    test_samples = test_data[np.random.choice(len(test_data), 512, False)][0]\n",
    "    test_samples = test_samples.to(next(model.parameters()).device)\n",
    "    ll = log_likelihood(generated_samples, valid_samples, test_samples)\n",
    "    log_likelihoods.append(ll)\n",
    "    print('Log-likelihood', ll, flush=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (batch, ) in enumerate(dataloader):\n",
    "            if torch.cuda.is_available():\n",
    "                batch = batch.cuda()\n",
    "\n",
    "            # ваш код здесь!\n",
    "            # разрешается менять код этой функции для реализации\n",
    "            # более сложных процедур обучения или ускорения обучения\n",
    "            pass\n",
    "\n",
    "            # не забудьте корректно сохранить статистику\n",
    "            discriminator_losses.append(float(discriminator_loss))\n",
    "            generator_losses.append(float(generator_loss))\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('\\rEpoch:', epoch, 'G_loss:', generator_losses[-1],\n",
    "                      'D_loss:', discriminator_losses[-1],\n",
    "                      'Batch', i + 1, 'of', total_batches,\n",
    "                      ' ' * 10, end='', flush=True)\n",
    "        print(flush=True)\n",
    "        generated_samples = generate_many_samples(model, 512, batch_size).detach()\n",
    "        valid_samples = valid_data[np.random.choice(len(valid_data), 512, False)][0]\n",
    "        valid_samples = valid_samples.to(next(model.parameters()).device)\n",
    "        test_samples = test_data[np.random.choice(len(test_data), 512, False)][0]\n",
    "        test_samples = test_samples.to(next(model.parameters()).device)\n",
    "        ll = log_likelihood(generated_samples, valid_samples, test_samples)\n",
    "        log_likelihoods.append(ll)\n",
    "        print('Log-likelihood', ll, flush=True)\n",
    "\n",
    "    return {\n",
    "        'model': model.cpu(),\n",
    "        'generator_losses': generator_losses,\n",
    "        'discriminator_losses': discriminator_losses,\n",
    "        'log_likelihoods': log_likelihoods\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсказка: обучать GANs с скрытой размерностью 2 непросто,\n",
    "# для этого требуется очень аккуратно подобрать хаки/процесс обучения/\n",
    "# /архитектуру модели/инициализацию.\n",
    "# Если не получается сразу, стоит обучить GANs с бОльшей скрытой размерностью,\n",
    "# а затем вернуться к этой ячейке.\n",
    "g2 = GAN(2)\n",
    "# возможно, для обучения модели достаточно/требуется другое число эпох\n",
    "%time gan_model_d2 = train_model(g2, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g10 = GAN(10)\n",
    "# возможно, для обучения модели достаточно/требуется другое число эпох\n",
    "%time gan_model_d10 = train_model(g10, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g100 = GAN(100)\n",
    "# возможно, для обучения модели достаточно/требуется другое число эпох\n",
    "%time gan_model_d100 = train_model(g100, num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_manifold_gan(model):\n",
    "    generator = lambda z: model.generator(torch.from_numpy(z).float()).view(digit_size, digit_size).data.numpy()\n",
    "    return draw_manifold(generator)\n",
    "\n",
    "draw_manifold_gan(gan_model_d2['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(gan_model_d2['model'].generate_samples(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(gan_model_d10['model'].generate_samples(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(gan_model_d100['model'].generate_samples(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "for label, name, model in [\n",
    "    ('$d = 2$, D_loss', 'discriminator_losses', gan_model_d2),\n",
    "    ('$d = 2$, G_loss', 'generator_losses', gan_model_d2),\n",
    "]:\n",
    "    data = model[name]\n",
    "    x_labels = (1 + np.arange(len(data))) / len(data) * 100\n",
    "    plt.plot(x_labels, data, label=label)\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(xmin=0.01, xmax=x_labels[-1])\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "for label, name, model in [\n",
    "    ('$d = 10$, D_loss', 'discriminator_losses', gan_model_d10),\n",
    "    ('$d = 10$, G_loss', 'generator_losses', gan_model_d10),\n",
    "]:\n",
    "    data = model[name]\n",
    "    x_labels = (1 + np.arange(len(data))) / len(data) * 100\n",
    "    plt.plot(x_labels, data, label=label)\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(xmin=0.01, xmax=x_labels[-1])\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "for label, name, model in [\n",
    "    ('$d = 100$, D_loss', 'discriminator_losses', gan_model_d100),\n",
    "    ('$d = 100$, G_loss', 'generator_losses', gan_model_d100),\n",
    "]:\n",
    "    data = model[name]\n",
    "    x_labels = (1 + np.arange(len(data))) / len(data) * 100\n",
    "    plt.plot(x_labels, data, label=label)\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(xmin=0.01, xmax=x_labels[-1])\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generate_many_samples(gan_model_d2['model'], 8192, 64).detach()\n",
    "%time log_likelihood(generated_samples, valid_data.tensors[0], test_data.tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generate_many_samples(gan_model_d10['model'], 8192, 64).detach()\n",
    "%time log_likelihood(generated_samples, valid_data.tensors[0], test_data.tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generate_many_samples(gan_model_d100['model'], 8192, 64).detach()\n",
    "%time log_likelihood(generated_samples, valid_data.tensors[0], test_data.tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "for label, name, model in [\n",
    "    ('$d = 2$, log-likelihoods', 'log_likelihoods', gan_model_d2),\n",
    "    ('$d = 10$, log-likelihoods', 'log_likelihoods', gan_model_d10),\n",
    "    ('$d = 100$, log-likelihoods', 'log_likelihoods', gan_model_d100),\n",
    "]:\n",
    "    data = model[name]\n",
    "    x_labels = np.arange(len(data))\n",
    "    plt.plot(x_labels, data, label=label)\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(xmin=0.0, xmax=x_labels[-1])\n",
    "plt.ylabel('Log-likelihood estimation')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Выводы\n",
    "Место для ваших выводов, наблюдений, гипотез."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
