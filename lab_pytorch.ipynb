{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть материала украдена из курса \"Глубинное обучение\" ФКН ВШЭ https://www.hse.ru/ba/ami/courses/205504078.html, за что им большое спасибо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Устанавливаем pytorch\n",
    "\n",
    "## Linux/OSX\n",
    "\n",
    "\n",
    "На оффсайте http://pytorch.org/ надо выбрать подходящую конфигурацию и скачать.\n",
    "\n",
    "Версию python можно узнать в терминале:\n",
    "```\n",
    "python --version\n",
    "```\n",
    "\n",
    "\n",
    "## Windows without GPU\n",
    "\n",
    "Проще всего поставить при помощи конды:\n",
    "```\n",
    "conda install -c peterjc123 pytorch\n",
    "```\n",
    "\n",
    "## Windows with GPU\n",
    "\n",
    "Смотрите https://github.com/peterjc123/pytorch-scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![img](https://s1.postimg.org/6fl45xnvnj/pytorch-logo-dark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numpy world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "print(\"X :\\n %s\" % x)\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n\", np.dot(x, x.T))\n",
    "print(\"mean over cols :\\n%s\" % (x.mean(axis=-1)))\n",
    "print(\"cumsum of cols :\\n%s\" % (np.cumsum(x, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pytorch world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "y = torch.from_numpy(x)\n",
    "print('Type of y:', type(y))\n",
    "y = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "print('Type of y', type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.arange(0,16).view(4,4)\n",
    "print('Type of x:', type(x))\n",
    "\n",
    "print(\"X :\\n%s\" % x)\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n\", torch.matmul(x, x.transpose(1, 0)))\n",
    "print(\"mean over cols :\\n\", torch.mean(x, dim=-1))\n",
    "print(\"cumsum of cols :\\n\", torch.cumsum(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy vs Pytorch\n",
    "\n",
    "Numpy и Pytorch не требуют описания статического графа вычислений. \n",
    "\n",
    "Можно отлаживаться с помощью pdb или просто print.\n",
    "\n",
    "API несколько различается:\n",
    "\n",
    "```\n",
    "x.reshape([1,2,8]) -> x.view(1,2,8)\n",
    "x.sum(axis=-1) -> x.sum(dim=-1)\n",
    "x.astype('int64') -> x.type(torch.LongTensor)\n",
    "```\n",
    "\n",
    "\n",
    "Легко конвертировать между собой:\n",
    "\n",
    "```\n",
    "torch.from_numpy(npx) -- вернет Tensor\n",
    "tt.numpy() -- вернет Numpy Array\n",
    "```\n",
    "\n",
    "\n",
    "Если что:\n",
    "- смотрите документацию\n",
    "- гуглите (Stackoverflow/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 2 * np.pi, 16)\n",
    "\n",
    "# Mini-task: compute a vector of sin^2(x) + cos^2(x)\n",
    "out = <your code>\n",
    "\n",
    "print(out.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда работаем с большими массивами, память надо экономить.\n",
    "Некоторые операции происходят с созданием нового объекта - результата вычислений,\n",
    "некоторые изменяют данный объект (in-place операции).\n",
    "В pytorch обычно эти операции различаются добавлением подчеркивания:\n",
    "```\n",
    "x.exp()   # not-in-place operation\n",
    "x.exp_()  # in-place operation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.arange(4)\n",
    "print('Not-in-place:')\n",
    "print('\\tx.exp():\\t\\t', x.exp().numpy())\n",
    "print('\\tx:\\t\\t\\t', x.numpy())\n",
    "print('In-place:')\n",
    "print('\\tx.exp_():\\t\\t', x.exp_().numpy())\n",
    "print('\\tx after x.exp_():\\t', x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.arange(0, 4).view(2, 2)\n",
    "y = torch.arange(4, 8).view(2, 2)\n",
    "z = torch.arange(8, 12).view(2, 2)\n",
    "\n",
    "# Not-in-place:\n",
    "u = x + 2 * y - z    # 3 array allocations?\n",
    "print(u.numpy())\n",
    "\n",
    "# In-place\n",
    "u = y.clone()        # 1 array allocation\n",
    "u.mul_(2)\n",
    "u.add_(x)\n",
    "u.sub_(z)\n",
    "print(u.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с тензорами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого объекта из X найти индекс ближайшего объекта из Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.randn(100, 10)\n",
    "Y = torch.randn(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "`x.cuda()` копирует тензор на GPU и возвращает объект, соответствующий этому скопированному тензору.\n",
    "Можно явно указать номер GPU, на который нужно скопировать тензор: `x.cuda(gpu_id)`.\n",
    "Если тензор уже лежал на нужном GPU, то возвращается сам тензор, копирования не производится.\n",
    "Аналогично работает `x.cpu()`. \n",
    "\n",
    "Операции можно осуществлять только над тензорами, лежащими на одном устройстве.\n",
    "Нарушение этого правила приводит к ошибке.\n",
    "Результат операции находится на том же устройстве, что и операнды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor vs Variable\n",
    "\n",
    "http://pytorch.org/docs/master/autograd.html#variable\n",
    "\n",
    "`Variable` – обертка над Tensor для использования в вычислительных графах. Позволяет вычислять градиенты автоматически.\n",
    "\n",
    "Tensor и Variable конвертируются друг в друга:\n",
    "```\n",
    "tensor to variable: Variable(x)\n",
    "variable to tensor: x.data\n",
    "```\n",
    "\n",
    "Нельзя смешивать Tensor и Variable в одной операции.\n",
    "\n",
    "Некоторые операции могут работать только с тензорами, некоторые только с переменными (torch.nn.functional.whatever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "sequence = torch.randn(1, 8, 10)\n",
    "filters = torch.randn(2, 8, 3)\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# works:\n",
    "print('conv1d over Variables:')\n",
    "print(torch.nn.functional.conv1d(Variable(sequence), Variable(filters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# works:\n",
    "print('sum of Variables:')\n",
    "print(Variable(a) + Variable(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will not work\n",
    "print(\"conv1d (tensors):\")\n",
    "print(torch.nn.functional.conv1d(sequence, filters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will not work:\n",
    "print('sum of Variable and Tensor:')\n",
    "print(Variable(a) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic gradients\n",
    "\n",
    "Автоматическое вычисление градиентов:\n",
    "\n",
    "1. Создать переменную: `a = Variable(..., requires_grad=True)`\n",
    "\n",
    "2. Определить какую-нибудь дифференцируемую функцию `loss = whatever(a)`\n",
    "\n",
    "3. Запросить обратный проход `loss.backward()`\n",
    "\n",
    "4. Градиенты будут доступны в `a.grads`\n",
    "\n",
    "\n",
    "Есть два важных отличия Pytorch от Theano/TF:\n",
    "\n",
    "1. Функцию ошибки можно изменять динамически, например на каждом минибатче.\n",
    "\n",
    "2. После вычисления `.backward()` градиенты сохраняются в `.grad` каждой задействованной переменной, при повторных вызовах градиенты суммируются. Это позволяет использовать несколько функций ошибок или виртуально увеличивать batch_size. Поэтому после каждого шага оптимизатора градиенты стоит обнулять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf vs Non-leaf Variable\n",
    "\n",
    "Градиенты будут сохранены и доступны для использования только для `leaf-variable`.\n",
    "Такое поведение по умолчанию сделано ради экономии памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)  # leaf variable\n",
    "y = x + 1  # not a leaf variable\n",
    "y.sum().backward()\n",
    "print('x.grad:', x.grad.data.numpy(), '\\ny.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Листовые вершины без градиентов\n",
    "Листовые вершины, в которых не требуется вычислять градиент, создаются с помощью `Variable(..., requires_grad=False)`.\n",
    "Для корректного вызова `.backward()` требуется, чтобы хотя бы для одной листовой вершины требовался градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=False)  # leaf variable\n",
    "z = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "y = x + z + 1  # not a leaf variable\n",
    "y.sum().backward()\n",
    "print('x.grad:', x.grad,\n",
    "      '\\nz.grad:', z.grad.data.numpy(),\n",
    "      '\\ny.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will not work:\n",
    "x = Variable(torch.randn(4), requires_grad=False)   # leaf variable\n",
    "z = Variable(torch.randn(4), requires_grad=False)   # leaf variable\n",
    "y = x + z + 1  # not a leaf variable\n",
    "y.sum().backward()\n",
    "print('x.grad:', x.grad,\n",
    "      '\\nz.grad:', z.grad.data.numpy(),\n",
    "      '\\ny.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиенты промежуточных вершин\n",
    "Для промежуточных вершин мы можем запросить сохранение градиентов с помощью функции `.retain_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "z = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "w = z * 2      # not a leaf variable\n",
    "y = x * w + 1  # forward pass before retaining gradient is ok\n",
    "\n",
    "w.retain_grad()\n",
    "\n",
    "y.sum().backward()\n",
    "print('d y_sum / dx:', x.grad.data.numpy(),\n",
    "      '\\nd y_sum / dw:', w.grad.data.numpy(),\n",
    "      '\\nd y_sum / dz:', z.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что даже при наличии в графе вычислений не-листовых вершин, требующих вычисления градиентов,\n",
    "`.backward()` выдает ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=False)   # leaf variable\n",
    "z = Variable(torch.randn(4), requires_grad=False)   # leaf variable\n",
    "w = z * 2      # not a leaf variable\n",
    "y = x * w + 1  # forward pass before retaining gradient is ok\n",
    "\n",
    "w.retain_grad()\n",
    "\n",
    "y.sum().backward()\n",
    "print('d y_sum / dx:', x.grad.data.numpy(),\n",
    "      '\\nd y_sum / dw:', w.grad.data.numpy(),\n",
    "      '\\nd y_sum / dz:', z.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отстреливаем себе ноги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертировать Variable в Tensor и обратно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x out of the computational graph\n",
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "z = torch.autograd.Variable(x.data * 2, requires_grad=True)\n",
    "y = 3 * z + 1\n",
    "y_mean = y.mean()\n",
    "y_mean.backward()\n",
    "print('d y_mean / dx:', x.grad, '\\nd y_sum / dz:', z.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Менять значения в Variable, но не обнулять градиенты (.grad.zero_() сохраняет размер, .grad = None не сохраняет)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "z = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = x * z + 1\n",
    "y.sum().backward()\n",
    "print('d y_sum / dx:', x.grad.data.numpy(), '\\nd y_sum / dz:', z.grad.data.numpy())\n",
    "\n",
    "#x.grad.zero_()\n",
    "#z.grad.zero_()\n",
    "x.grad = None\n",
    "z.grad = None\n",
    "\n",
    "z.data = torch.Tensor([1, 2, 3])\n",
    "x.data = torch.Tensor([1, 2, 3])\n",
    "y = x * z + 1\n",
    "y.sum().backward()\n",
    "print('d y_sum / dx:', x.grad.data.numpy(), '\\nd y_sum / dz:', z.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Менять значения Variable после вычисления каких-то других выражений с ним и рассчитывать,\n",
    "что градиент от тех выражений будет учитывать новое значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "z = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "w = z ** 2\n",
    "\n",
    "w.data = torch.Tensor([1, 2, 3, 4])  # changing .data before computation matters\n",
    "y = x * w + 1\n",
    "x.data = torch.Tensor([1, 1, 1])     # changing .data after computation doesn't affect gradients\n",
    "\n",
    "y.sum().backward()\n",
    "print('d y_sum / dx:', x.grad.data.numpy(), '\\nd y_sum / dz:', z.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тысячи их!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.randn(50, 10)\n",
    "b = torch.randn(2)\n",
    "W = torch.randn(10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наркоманская функция потерь, вынуждающая линейное преобразование переводить точки из многомерного пространства в двумерное на единичную окружность. Для оптимизации использовать градиентный спуск.\n",
    "$$y = Wx + b$$\n",
    "$$||y||_2 = \\sqrt{y_1^2 + y_2^2}$$\n",
    "$$f(x, W, b) = \\big| ||y||_2 - 1 \\big| + max\\big(0, \\frac{1}{||y - b||_2} - 1\\big)$$\n",
    "$$\\frac{1}{N}\\sum\\limits_{i = 1}^N f(x_i, W, b) \\to \\min\\limits_{W, b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(X, W, b):\n",
    "    <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(f(X, W, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "Y = X.mm(W).add(b)\n",
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "Y = X.mm(W).add(b)\n",
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Время писать нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ячейке ниже чего-то не хватает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем слои нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = <your code here>\n",
    "        self.biases = <your code here>\n",
    "        self.params = [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        <your code here>\n",
    "        return output\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Set layer into train mode (for DropOut, BatchNorm, etc)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set layer into evaluation mode\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        ReLU layer simply applies elementwise rectified linear unit to all inputs\n",
    "        \"\"\"\n",
    "        self.params = [] # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Apply elementwise ReLU to [batch, num_units] matrix\n",
    "        \"\"\"\n",
    "        <your code here>\n",
    "        return output\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Set layer into train mode (for DropOut, BatchNorm, etc)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set layer into evaluation mode\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        <your code here>\n",
    "        return output\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Set layer into train mode (for DropOut, BatchNorm, etc)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set layer into evaluation mode\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes)\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: 1 (scalar)\n",
    "    \"\"\"\n",
    "    <your code here>\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = []\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, params, grads):\n",
    "        \"\"\"\n",
    "        Take list of parameters and list of their gradients,\n",
    "        make one optimization step for parameters in-place.\n",
    "        \"\"\"\n",
    "        <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_data, batch_target):\n",
    "    \"\"\"\n",
    "    Make forward pass through network,\n",
    "    compute gradients w. r. t. parameters,\n",
    "    return loss.\n",
    "    \"\"\"\n",
    "    <your code here>\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(dataset, prefix='Test loss:', optimizer=None):\n",
    "    # Change mode for all layers.\n",
    "    for layer in network:\n",
    "        if optimizer:\n",
    "            layer.train()\n",
    "        else:\n",
    "            layer.eval()\n",
    "\n",
    "    batch_size = 100\n",
    "    batchgenerator = torch.utils.data.DataLoader(dataset, batch_size, True)\n",
    "\n",
    "    avg_loss = 0\n",
    "    for i, (batch_data, batch_target) in enumerate(batchgenerator):\n",
    "        avg_loss += (process_batch(batch_data, batch_target) - avg_loss) / (i + 1)\n",
    "        if optimizer:\n",
    "            optimizer.step(\n",
    "                [param.data for layer in network for param in layer.params],\n",
    "                [param.grad.data for layer in network for param in layer.params]\n",
    "            )\n",
    "            for layer in network:\n",
    "                for param in layer.params:\n",
    "                    param.grad.data.zero_()\n",
    "    print(prefix, avg_loss, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "sgd = SGDOptimizer(0.5)\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, 'Train loss:', sgd)\n",
    "    run_epoch(test_dataset,  'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Различные модификации/улучшения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RMSPropOptimizer:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, params, grads):\n",
    "        \"\"\"\n",
    "        Take list of parameters and list of their gradients,\n",
    "        make one optimization step for parameters in-place.\n",
    "        \"\"\"\n",
    "        <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, params, grads):\n",
    "        \"\"\"\n",
    "        Take list of parameters and list of their gradients,\n",
    "        make one optimization step for parameters in-place.\n",
    "        \"\"\"\n",
    "        <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseWithDropOut:\n",
    "    def __init__(self, input_units, output_units, dropout_rate):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned\n",
    "        affine transformation and applies dropout:\n",
    "        f(x) = (W x + b) o Bernoulli(p, size=output_units)\n",
    "        \"\"\"\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.train_mode = True\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = <your code here>\n",
    "        self.biases = <your code here>\n",
    "        self.params = [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation with dropout.\n",
    "        In training mode:\n",
    "        f(x) = (W x + b) o Bernoulli(p, size=output_units)\n",
    "        In evaluation mode:\n",
    "        f(x) = (W x + b) p \n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        <your code here>\n",
    "        return output\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Set layer into train mode (for DropOut, BatchNorm, etc)\n",
    "        \"\"\"\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set layer into evaluation mode\n",
    "        \"\"\"\n",
    "        self.train_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchNormLayer:\n",
    "    def __init__(self, input_units, output_units, dropout_rate):\n",
    "        \"\"\"\n",
    "        TBD\n",
    "        \"\"\"\n",
    "        self.train_mode = True\n",
    "        <your code here>\n",
    "        self.params = [<your code here>]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        TBD\n",
    "        \"\"\"\n",
    "        <your code here>\n",
    "        return output\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Set layer into train mode (for DropOut, BatchNorm, etc)\n",
    "        \"\"\"\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set layer into evaluation mode\n",
    "        \"\"\"\n",
    "        self.train_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, верно ли, что полносвязная сеть с dropout работает быстрее, чем обычная полносвязная сеть,\n",
    "засчет динамического графа вычислений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network1 = [DenseWithDropOut(1000, 1000, 0.9), Dense(1000, 1)]\n",
    "network2 = [Dense(1000, 1000), Dense(1000, 1)]\n",
    "X = torch.randn(100, 1000)\n",
    "# check whether DenseWithDropOut works faster than Dense\n",
    "<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что достаточно глубокая сеть не обучается.\n",
    "\n",
    "Изучаем влияние различных методов регуляризации и оптимизации на обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = []\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "for i in range(10):\n",
    "    network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "    network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "sgd = SGDOptimizer(0.5)\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, 'Train loss:', sgd)\n",
    "    run_epoch(test_dataset,  'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
