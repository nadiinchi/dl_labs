{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть материала украдена из курса \"Глубинное обучение\" ФКН ВШЭ https://www.hse.ru/ba/ami/courses/205504078.html, за что им большое спасибо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Актуальная версия этого ноутбука обретается по адресу\n",
    "https://github.com/nadiinchi/dl_labs/blob/master/lab_pytorch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Устанавливаем pytorch\n",
    "\n",
    "## Linux/OSX\n",
    "\n",
    "\n",
    "На оффсайте http://pytorch.org/ надо выбрать подходящую конфигурацию и скачать.\n",
    "\n",
    "Версию python можно узнать в терминале:\n",
    "```\n",
    "python --version\n",
    "```\n",
    "\n",
    "\n",
    "## Windows without GPU\n",
    "\n",
    "Проще всего поставить при помощи конды:\n",
    "```\n",
    "conda install -c peterjc123 pytorch\n",
    "```\n",
    "\n",
    "## Windows with GPU\n",
    "\n",
    "Смотрите https://github.com/peterjc123/pytorch-scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![img](https://s1.postimg.org/6fl45xnvnj/pytorch-logo-dark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :\n",
      " [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "add 5 :\n",
      "[[ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [13 14 15 16]\n",
      " [17 18 19 20]]\n",
      "X*X^T  :\n",
      " [[ 14  38  62  86]\n",
      " [ 38 126 214 302]\n",
      " [ 62 214 366 518]\n",
      " [ 86 302 518 734]]\n",
      "mean over cols :\n",
      "[  1.5   5.5   9.5  13.5]\n",
      "cumsum of cols :\n",
      "[[ 0  1  2  3]\n",
      " [ 4  6  8 10]\n",
      " [12 15 18 21]\n",
      " [24 28 32 36]]\n"
     ]
    }
   ],
   "source": [
    "# numpy world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "print(\"X :\\n %s\" % x)\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n\", np.dot(x, x.T))\n",
    "print(\"mean over cols :\\n%s\" % (x.mean(axis=-1)))\n",
    "print(\"cumsum of cols :\\n%s\" % (np.cumsum(x, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of y: <class 'torch.LongTensor'>\n",
      "Type of y <class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "# pytorch world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "y = torch.from_numpy(x)\n",
    "print('Type of y:', type(y))\n",
    "y = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "print('Type of y', type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of x: <class 'torch.FloatTensor'>\n",
      "X :\n",
      "\n",
      "  0   1   2   3\n",
      "  4   5   6   7\n",
      "  8   9  10  11\n",
      " 12  13  14  15\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "add 5 :\n",
      "\n",
      "  5   6   7   8\n",
      "  9  10  11  12\n",
      " 13  14  15  16\n",
      " 17  18  19  20\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "X*X^T  :\n",
      " \n",
      "  14   38   62   86\n",
      "  38  126  214  302\n",
      "  62  214  366  518\n",
      "  86  302  518  734\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "mean over cols :\n",
      " \n",
      "  1.5000\n",
      "  5.5000\n",
      "  9.5000\n",
      " 13.5000\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "cumsum of cols :\n",
      " \n",
      "  0   1   2   3\n",
      "  4   6   8  10\n",
      " 12  15  18  21\n",
      " 24  28  32  36\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,16).view(4,4)\n",
    "print('Type of x:', type(x))\n",
    "\n",
    "print(\"X :\\n%s\" % x)\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n\", torch.matmul(x, x.transpose(1, 0)))\n",
    "print(\"mean over cols :\\n\", torch.mean(x, dim=-1))\n",
    "print(\"cumsum of cols :\\n\", torch.cumsum(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy vs Pytorch\n",
    "\n",
    "Numpy и Pytorch не требуют описания статического графа вычислений. \n",
    "\n",
    "Можно отлаживаться с помощью pdb или просто print.\n",
    "\n",
    "API несколько различается:\n",
    "\n",
    "```\n",
    "x.reshape([1,2,8]) -> x.view(1,2,8)\n",
    "x.sum(axis=-1) -> x.sum(dim=-1)\n",
    "x.astype('int64') -> x.type(torch.LongTensor)\n",
    "```\n",
    "\n",
    "\n",
    "Легко конвертировать между собой:\n",
    "\n",
    "```\n",
    "torch.from_numpy(npx) -- вернет Tensor\n",
    "tt.numpy() -- вернет Numpy Array\n",
    "```\n",
    "\n",
    "\n",
    "Если что:\n",
    "- смотрите документацию\n",
    "- гуглите (Stackoverflow/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(0, 2 * np.pi, 16)\n",
    "\n",
    "# Mini-task: compute a vector of sin^2(x) + cos^2(x)\n",
    "out = torch.sin(x)**2 + torch.cos(x)**2\n",
    "\n",
    "print(out.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда работаем с большими массивами, память надо экономить.\n",
    "Некоторые операции происходят с созданием нового объекта – результата вычислений,\n",
    "некоторые изменяют данный объект (in-place операции).\n",
    "В pytorch обычно эти операции различаются добавлением подчеркивания:\n",
    "```\n",
    "x.exp()   # not-in-place operation\n",
    "x.exp_()  # in-place operation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not-in-place:\n",
      "\tx.exp():\t\t [  1.           2.71828175   7.38905621  20.08553696]\n",
      "\tx:\t\t\t [ 0.  1.  2.  3.]\n",
      "In-place:\n",
      "\tx.exp_():\t\t [  1.           2.71828175   7.38905621  20.08553696]\n",
      "\tx after x.exp_():\t [  1.           2.71828175   7.38905621  20.08553696]\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "print('Not-in-place:')\n",
    "print('\\tx.exp():\\t\\t', x.exp().numpy())\n",
    "print('\\tx:\\t\\t\\t', x.numpy())\n",
    "print('In-place:')\n",
    "print('\\tx.exp_():\\t\\t', x.exp_().numpy())\n",
    "print('\\tx after x.exp_():\\t', x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  2.]\n",
      " [ 4.  6.]]\n",
      "[[ 0.  2.]\n",
      " [ 4.  6.]]\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 4).view(2, 2)\n",
    "y = torch.arange(4, 8).view(2, 2)\n",
    "z = torch.arange(8, 12).view(2, 2)\n",
    "\n",
    "# Not-in-place:\n",
    "u = x + 2 * y - z    # 3 array allocations?\n",
    "print(u.numpy())\n",
    "\n",
    "# In-place\n",
    "u = y.clone()        # 1 array allocation\n",
    "u.mul_(2)\n",
    "u.add_(x)\n",
    "u.sub_(z)\n",
    "print(u.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting на pytorch (аналогично numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 2\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n",
      "b: \n",
      " 1  0  1  0\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "a + b: \n",
      " 2  1  2  1\n",
      " 2  1  2  1\n",
      " 2  1  2  1\n",
      " 3  2  3  2\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "c: \n",
      "-1.5156  0.4265  0.3472 -0.6855\n",
      " 0.7468 -0.5077  0.4843 -0.2762\n",
      " 1.1128 -0.6938 -0.0897  0.0438\n",
      " 0.0192  0.7684  1.1240 -0.3300\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "b + c: \n",
      "-0.5156  0.4265  1.3472 -0.6855\n",
      " 1.7468 -0.5077  1.4843 -0.2762\n",
      " 2.1128 -0.6938  0.9103  0.0438\n",
      " 1.0192  0.7684  2.1240 -0.3300\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1, 1, 1, 2]).view(4, 1)\n",
    "b = torch.Tensor([1, 0, 1, 0]).view(1, 4)\n",
    "c = torch.randn(16).view(4, 4)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('a + b:', a + b)\n",
    "print('c:', c)\n",
    "print('b + c:', b + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более подробную информацию можно найти на http://pytorch.org/docs/master/notes/broadcasting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с тензорами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано 100 объектов, каждый из которых описывается 10-мерным вектором, и 5 точек, каждая из которых также задается 10-мерным вектором. Объекты лежат в матрице X, точки – в матрице Y.\n",
    "\n",
    "Надо для каждого объекта из X найти индекс ближайшей точки из Y только с помощью операций над тензорами\n",
    "(нельзя использовать циклы, list comprehensions, рекурсию, etc,\n",
    "потому что решение с ними будет работать в несколько раз или на несколько порядков медленнее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.randn(100, 10)\n",
    "Y = torch.randn(5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение с семинара:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 2, 1, 2, 3, 1, 3, 1, 1, 1, 4, 1, 4, 4, 1, 1, 1, 1, 3, 3,\n",
       "       0, 1, 2, 1, 2, 1, 2, 0, 1, 2, 0, 1, 0, 2, 2, 0, 4, 2, 2, 2, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 4, 2, 0, 2, 1, 2, 2, 2, 0, 1, 1, 1, 1, 1, 2, 1,\n",
       "       2, 3, 2, 0, 3, 3, 2, 2, 1, 4, 1, 4, 1, 2, 0, 4, 2, 0, 3, 1, 1, 0, 4,\n",
       "       2, 2, 2, 2, 2, 3, 1, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((X.view(100, 1, 10) - Y.view(1, 5, 10)) ** 2).sum(dim=-1).min(dim=-1)[1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это решение плохо тем, что в качестве промежуточного результата вычилений в нем присутствует трехмерный тензор,\n",
    "который занимает $O(NMD)$ памяти, где N – число объектов, M – число точек, D – размерность пространства."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Внимание, задача!\n",
    "Утверждается, что есть другое решение с такой же скоростью работы,\n",
    "но использующее $O(NM)$ памяти для результатов промежуточных вычислений.\n",
    "Предлагается найти его.\n",
    "\n",
    "Подсказка: найти матрицу попарных скалярных произведений между объектами\n",
    "и точками можно с помощью одного матричного умножения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 2, 1, 2, 3, 1, 3, 1, 1, 1, 4, 1, 4, 4, 1, 1, 1, 1, 3, 3,\n",
       "       0, 1, 2, 1, 2, 1, 2, 0, 1, 2, 0, 1, 0, 2, 2, 0, 4, 2, 2, 2, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 4, 2, 0, 2, 1, 2, 2, 2, 0, 1, 1, 1, 1, 1, 2, 1,\n",
       "       2, 3, 2, 0, 3, 3, 2, 2, 1, 4, 1, 4, 1, 2, 0, 4, 2, 0, 3, 1, 1, 0, 4,\n",
       "       2, 2, 2, 2, 2, 3, 1, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "`x.cuda()` копирует тензор на GPU и возвращает объект, соответствующий этому скопированному тензору.\n",
    "Можно явно указать номер GPU, на который нужно скопировать тензор: `x.cuda(gpu_id)`.\n",
    "Если тензор уже лежал на нужном GPU, то возвращается сам тензор, копирования не производится.\n",
    "Аналогично работает `x.cpu()`. \n",
    "\n",
    "Операции можно осуществлять только над тензорами, лежащими на одном устройстве.\n",
    "Нарушение этого правила приводит к ошибке.\n",
    "Результат операции находится на том же устройстве, что и операнды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor vs Variable\n",
    "\n",
    "http://pytorch.org/docs/master/autograd.html#variable\n",
    "\n",
    "`Variable` – обертка над Tensor для использования в вычислительных графах. Позволяет вычислять градиенты автоматически.\n",
    "\n",
    "Tensor и Variable конвертируются друг в друга:\n",
    "```\n",
    "tensor to variable: Variable(x)\n",
    "variable to tensor: x.data\n",
    "```\n",
    "\n",
    "Нельзя смешивать Tensor и Variable в одной операции.\n",
    "\n",
    "Некоторые операции могут работать только с тензорами, некоторые только с переменными (torch.nn.functional.whatever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "sequence = torch.randn(1, 8, 10)\n",
    "filters = torch.randn(2, 8, 3)\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of Variables:\n",
      "Variable containing:\n",
      " 0.9240\n",
      " 0.9564\n",
      " 4.3621\n",
      "-1.7138\n",
      " 0.0694\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# works:\n",
    "print('sum of Variables:')\n",
    "print(Variable(a) + Variable(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of Variable and Tensor:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "add() received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float other, float alpha)\n * (Variable other, float alpha)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2fb7f3cf0dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# will not work:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum of Variable and Tensor:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: add() received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float other, float alpha)\n * (Variable other, float alpha)\n"
     ]
    }
   ],
   "source": [
    "# will not work:\n",
    "print('sum of Variable and Tensor:')\n",
    "print(Variable(a) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d over Variables:\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -6.3957 -0.9862  1.7453  5.0739 -5.6403 -0.8131 -1.6775 -3.7941\n",
      " -0.0211 -3.5564 -4.0188  6.0429 -2.6536  1.4192 -0.7251 -3.7977\n",
      "[torch.FloatTensor of size 1x2x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# works:\n",
    "print('conv1d over Variables:')\n",
    "print(torch.nn.functional.conv1d(Variable(sequence), Variable(filters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d (tensors):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 0 is not a Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b8774cf17ce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# will not work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1d (tensors):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/soft/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0m_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 0 is not a Variable"
     ]
    }
   ],
   "source": [
    "# will not work\n",
    "print(\"conv1d (tensors):\")\n",
    "print(torch.nn.functional.conv1d(sequence, filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic gradients\n",
    "\n",
    "Автоматическое вычисление градиентов:\n",
    "\n",
    "1. Создать переменную: `a = Variable(..., requires_grad=True)`\n",
    "\n",
    "2. Определить какую-нибудь дифференцируемую _скалярную_ функцию `loss = whatever(a)`\n",
    "\n",
    "3. Запросить обратный проход `loss.backward()`\n",
    "\n",
    "4. Градиенты будут доступны в `a.grads`\n",
    "\n",
    "\n",
    "Есть два важных отличия Pytorch от Theano/TF:\n",
    "\n",
    "1. Функцию ошибки можно изменять динамически, например на каждом минибатче.\n",
    "\n",
    "2. После вычисления `.backward()` градиенты сохраняются в `.grad` каждой задействованной переменной, при повторных вызовах градиенты суммируются. Это позволяет использовать несколько функций ошибок или виртуально увеличивать batch_size. Поэтому после каждого шага оптимизатора градиенты стоит обнулять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простой пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " 1.4730\n",
      "-1.7276\n",
      " 0.7120\n",
      " 0.9192\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "y: \n",
      " 0.9468\n",
      "-1.2150\n",
      "-1.5024\n",
      " 0.6761\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "dp / dx: \n",
      " 0.9468\n",
      "-1.2150\n",
      "-1.5024\n",
      " 0.6761\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "dp / dy: \n",
      " 1.4730\n",
      "-1.7276\n",
      " 0.7120\n",
      " 0.9192\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_tensor = torch.randn(4)\n",
    "y_tensor = torch.randn(4)\n",
    "x = Variable(x_tensor, requires_grad=True)\n",
    "y = Variable(y_tensor, requires_grad=True)\n",
    "z = x * y + 10\n",
    "p = z.sum()\n",
    "p.backward()\n",
    "print('x:', x.data)\n",
    "print('y:', y.data)\n",
    "print('dp / dx:', x.grad.data)\n",
    "print('dp / dy:', y.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обнуление градиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 1.  1.  1.  1.]\n",
      "dp / dx: [ 2.  2.  2.  2.]\n",
      "x: [ 0.5  0.5  0.5  0.5]\n",
      "dp / dx: [-2. -2. -2. -2.]\n"
     ]
    }
   ],
   "source": [
    "x_tensor = torch.Tensor([1, 1, 1, 1])\n",
    "x = Variable(x_tensor, requires_grad=True)\n",
    "y = x ** 2\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())\n",
    "x.data -= 0.5\n",
    "y = 1 / x\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 1.  1.  1.  1.]\n",
      "dp / dx: [ 2.  2.  2.  2.]\n",
      "x: [ 0.5  0.5  0.5  0.5]\n",
      "dp / dx: [-4. -4. -4. -4.]\n"
     ]
    }
   ],
   "source": [
    "x_tensor = torch.Tensor([1, 1, 1, 1])\n",
    "x = Variable(x_tensor, requires_grad=True)\n",
    "y = x ** 2\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())\n",
    "x.grad.detach_()       # extracting gradient Variable from the previous computational graph (optional)\n",
    "x.grad.data.zero_()    # zero gradinents\n",
    "x.data -= 0.5\n",
    "y = 1 / x\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf vs Non-leaf Variable\n",
    "\n",
    "Градиенты будут сохранены и доступны для использования только для `leaf-variable`.\n",
    "Такое поведение по умолчанию сделано ради экономии памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.data: [-1.09473073 -0.83525252  1.05651534 -0.73443204]\n",
      "y.data: [-0.09473073  0.16474748  2.05651522  0.26556796]\n",
      "p.data: [ 2.39209986]\n",
      "x.grad: [ 1.  1.  1.  1.]\n",
      "y.grad: None\n",
      "p.grad: None\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)  # leaf variable\n",
    "y = x + 1                                         # not a leaf variable\n",
    "p = y.sum()                                       # not a leaf variable\n",
    "p.backward()\n",
    "print('x.data:', x.data.numpy())\n",
    "print('y.data:', y.data.numpy())\n",
    "print('p.data:', p.data.numpy())\n",
    "print('x.grad:', x.grad.data.numpy())\n",
    "print('y.grad:', y.grad)\n",
    "print('p.grad:', p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: [ 1.  1.  1.  1.]\n",
      "y.grad: [ 1.  1.  1.  1.]\n",
      "z.grad: None\n",
      "p.grad: None\n",
      "x.is_leaf: True\n",
      "y.is_leaf: True\n",
      "z.is_leaf: False\n",
      "p.is_leaf: False\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)  # leaf variable\n",
    "y = Variable(torch.randn(4), requires_grad=True)  # leaf variable\n",
    "z = x + y    # not a leaf variable\n",
    "p = z.sum()  # not a leaf variable\n",
    "p.backward()\n",
    "print('x.grad:', x.grad.data.numpy())\n",
    "print('y.grad:', y.grad.data.numpy())\n",
    "print('z.grad:', z.grad)\n",
    "print('p.grad:', p.grad)\n",
    "print('x.is_leaf:', x.is_leaf)\n",
    "print('y.is_leaf:', y.is_leaf)\n",
    "print('z.is_leaf:', z.is_leaf)\n",
    "print('p.is_leaf:', p.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Листовые вершины без градиентов\n",
    "Листовые вершины, в которых не требуется вычислять градиент, создаются с помощью `Variable(..., requires_grad=False)`.\n",
    "Для корректного вызова `.backward()` требуется, чтобы хотя бы для одной листовой вершины требовался градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: [ 1.  1.  1.  1.]\n",
      "y.grad: None\n",
      "z.grad: None\n",
      "p.grad: None\n",
      "x.is_leaf: True\n",
      "y.is_leaf: True\n",
      "z.is_leaf: False\n",
      "p.is_leaf: False\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "y = Variable(torch.randn(4), requires_grad=False)  # leaf variable\n",
    "z = x + y    # not a leaf variable\n",
    "p = z.sum()  # not a leaf variable\n",
    "p.backward()\n",
    "print('x.grad:', x.grad.data.numpy())\n",
    "print('y.grad:', y.grad)\n",
    "print('z.grad:', z.grad)\n",
    "print('p.grad:', p.grad)\n",
    "print('x.is_leaf:', x.is_leaf)\n",
    "print('y.is_leaf:', y.is_leaf)\n",
    "print('z.is_leaf:', z.is_leaf)\n",
    "print('p.is_leaf:', p.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что для вычисления градиента нужно, чтобы хотя бы одна листовая вершина графа вычисления функции\n",
    "имела `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of variables does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1a8a06cd070e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m    \u001b[0;31m# not a leaf variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# not a leaf variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/soft/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/soft/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of variables does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# will not work:\n",
    "x = Variable(torch.randn(4), requires_grad=False)  # leaf variable\n",
    "y = Variable(torch.randn(4), requires_grad=False)  # leaf variable\n",
    "z = x + y    # not a leaf variable\n",
    "p = z.sum()  # not a leaf variable\n",
    "p.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиенты промежуточных вершин\n",
    "Для промежуточных вершин мы можем запросить сохранение градиентов с помощью функции `.retain_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp / dx: [ 1.92757583 -2.2354784   1.24162018  2.519238  ]\n",
      "dp / dw: [ 0.79092884 -0.35837969 -1.23274362  0.46698242]\n",
      "dp / dz: [ 1.58185768 -0.71675938 -2.46548724  0.93396485]\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "z = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "w = z * 2      # not a leaf variable\n",
    "y = x * w + 1  # forward pass before retaining gradient is ok\n",
    "p = y.sum()\n",
    "\n",
    "w.retain_grad()\n",
    "\n",
    "p.backward()\n",
    "print('dp / dx:', x.grad.data.numpy())\n",
    "print('dp / dw:', w.grad.data.numpy())\n",
    "print('dp / dz:', z.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что даже при наличии в графе вычислений не-листовых вершин, требующих вычисления градиентов,\n",
    "`.backward()` выдает ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of variables does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-03d569ff82b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/soft/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/soft/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of variables does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# will not work\n",
    "x = Variable(torch.randn(4), requires_grad=False)   # leaf variable\n",
    "z = Variable(torch.randn(4), requires_grad=False)   # leaf variable\n",
    "w = z * 2      # not a leaf variable\n",
    "y = x * w + 1  # forward pass before retaining gradient is ok\n",
    "p = y.sum()\n",
    "\n",
    "w.retain_grad()\n",
    "\n",
    "p.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отстреливаем себе ноги (НЕ НАДО так делать)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертировать Variable в Tensor и обратно:\n",
    "backward pass не проходит через Tensor, даже если он был сконвертирован из другого Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp / dx: None\n",
      "dp / dy: [ 0.75  0.75  0.75  0.75]\n"
     ]
    }
   ],
   "source": [
    "# x out of the computational graph\n",
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(x.data * 2, requires_grad=True)   # the bad conversion is here\n",
    "z = 3 * y + 1\n",
    "p = z.mean()\n",
    "p.backward()\n",
    "print('dp / dx:', x.grad)\n",
    "print('dp / dy:', y.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Менять размерность тензоров в Variable, но не обнулять градиенты (`.grad.zero_()` сохраняет размер, `.grad = None` не сохраняет)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz / dx: [ 1.  2.  3.  4.]\n",
      "dz / dy: [ 1.  2.  3.  4.]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'zero_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5aa15072c346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#x.grad = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#z.grad = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_'"
     ]
    }
   ],
   "source": [
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "z = x * y + 1\n",
    "z.sum().backward()\n",
    "print('dz / dx:', x.grad.data.numpy())\n",
    "print('dz / dy:', y.grad.data.numpy())\n",
    "\n",
    "x.grad.zero_()\n",
    "z.grad.zero_()\n",
    "#x.grad = None\n",
    "#z.grad = None\n",
    "\n",
    "x.data = torch.Tensor([1, 2, 3])\n",
    "y.data = torch.Tensor([1, 2, 3])\n",
    "z = x * y + 1\n",
    "z.sum().backward()\n",
    "print('dz / dx:', x.grad.data.numpy())\n",
    "print('dz / dy:', y.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Менять значения Variable после вычисления каких-то других выражений с ним и рассчитывать,\n",
    "что градиент от тех выражений будет учитывать новое значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d p_sum / dx: [ 1.  2.  3.  4.]\n",
      "d p_sum / dy: [  2.   8.  18.  32.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "z = y ** 2\n",
    "\n",
    "z.data = torch.Tensor([1, 2, 3, 4])  # changing .data before computation matters\n",
    "p = x * z\n",
    "x.data = torch.Tensor([1, 1, 1])     # changing .data after computation doesn't affect gradients\n",
    "\n",
    "p.sum().backward()\n",
    "print('d p_sum / dx:', x.grad.data.numpy())\n",
    "print('d p_sum / dy:', y.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тысячи способов прострелить себе ногу, если использовать механизм автоматического дифференцирования\n",
    "любым другим нетрадиционным образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.randn(50, 10)\n",
    "b = torch.randn(2)\n",
    "W = torch.randn(10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наркоманская функция потерь, вынуждающая линейное преобразование переводить точки из многомерного пространства в двумерное на единичную окружность. Для оптимизации использовать градиентный спуск по параметрам преобразования.\n",
    "\n",
    "Линейное преобразование точки $x$ из десятимерного пространства в точку $y$ двумерного пространства с весами преобразования $W$ и $b$:\n",
    "$$y = Wx + b$$\n",
    "\n",
    "Норма в двумерном пространстве – евклидова:\n",
    "$$||y||_2 = \\sqrt{y_1^2 + y_2^2}$$\n",
    "\n",
    "Функция потерь $f_0$ штрафует расстояние от получившейся точки $y$ до единичной окружности:\n",
    "$$f_0(x, W, b) = 0.5 \\cdot \\big| ||y||_2 - 1 \\big| + \\big( ||y||_2 - 1 \\big)^2$$\n",
    "\n",
    "К сожалению, оптимизация функции $f_0$ по $W$ и $b$ может быть проведена аналитически\n",
    "и приводит к тривиальному решению $W = 0$, $b = (1, 0)$.\n",
    "Чтобы избежать такого решения, вводим штраф на близость получившейся точки к вектору $b$, который обращается в 0, если расстояние до вектора $b$ более 1:\n",
    "$$f_1(x, W, b) = \\max\\big(0, \\frac{1}{||y - b||_2} - 1\\big)$$\n",
    "\n",
    "Итоговая функция потерь:\n",
    "$$f(x, W, b) = f_0(x, W, b) + f_1(x, W, b)$$\n",
    "\n",
    "Нужно решить следующую оптимизационную задачу:\n",
    "$$\\frac{1}{N}\\sum\\limits_{i = 1}^N f(x_i, W, b) \\to \\min\\limits_{W, b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(X, W, b):\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.122261142730714\n"
     ]
    }
   ],
   "source": [
    "print(f(X, W, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7fe8037048>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEyCAYAAABj+rxLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7fe8f6bda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "Y = X.mm(W).add(b)\n",
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842589\n",
      "0.363394\n",
      "0.307726\n",
      "0.278642\n",
      "0.262647\n",
      "0.244216\n",
      "0.222634\n",
      "0.205349\n",
      "0.197059\n",
      "0.193335\n"
     ]
    }
   ],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7fe22b0320>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEyCAYAAACYrUmUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHzNV2ksxZytnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7fe22cb550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "Y = X.mm(W).add(b)\n",
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Время писать нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсказка: нейросети крайне плохо обучаются, если подаваемые им на вход значения велики по модулю.\n",
    "Поэтому перед обучением нейросети каждый признак независимо нормируют\n",
    "(исключение – сверточные нейросети, там нормируют изображение поканально, а не попиксельно, но об этом потом).\n",
    "\n",
    "Можно использовать разные нормировки.\n",
    "Наиболее популярно вычитать среднее и делить на дисперсию (нужно внимательно подходить к этому методу,\n",
    "когда выборочная дисперсия мала или равна нулю, и обрабатывать такие случаи отдельно).\n",
    "Можно также вычитать медиану и делить на интерквартильный размах, масштабировать все данные в отрезок $[-1, 1]$, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно реализовать свою нормировку данных здесь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем слои нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "        self.children = []\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns list of parameters of module and its children.\"\"\"\n",
    "        res = []\n",
    "        for submodule in self.children:\n",
    "            res += submodule.parameters()\n",
    "        for param in res:\n",
    "            if not isinstance(param, Variable):\n",
    "                raise Exception('Parameters must be Variables.')\n",
    "        return res\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Sets gradients of all model parameters to zero.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()   # detachs gradient Variable from the computational graph\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Sets module into train mode (for DropOut, BatchNorm, etc).\"\"\"\n",
    "        self.training = True\n",
    "        for submodule in self.children:\n",
    "            submodule.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Sets module into evaluation mode.\"\"\"\n",
    "        self.training = False\n",
    "        for submodule in self.children:\n",
    "            submodule.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Module):\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        super(Dense, self).__init__()\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = None  # your code here\n",
    "        self.biases = None   # your code here\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Performs an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        input shape:  [batch, input_units]  (Variable)\n",
    "        output shape: [batch, output units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs.\"\"\"\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def parameters(self):\n",
    "        return []  # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies elementwise ReLU to [batch, num_units] Variable matrix.\"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogSoftmax(Module):\n",
    "    def __init__(self):\n",
    "        super(LogSoftmax, self).__init__()\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies softmax to each row and then applies component-wise log.\n",
    "        Input shape:  [batch, num_units] (Variable)\n",
    "        Output shape: [batch, num_units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyNetwork(Module):\n",
    "    def __init__(self, input_size, hidden_layers_size, hidden_layers_number, output_size):\n",
    "        super(MyNetwork, self).__init__()\n",
    "\n",
    "        network = []\n",
    "        network.append(Dense(input_size, hidden_layers_size))\n",
    "        network.append(ReLU())\n",
    "        for i in range(hidden_layers_number - 1):\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "        network.append(Dense(hidden_layers_size, output_size))\n",
    "        network.append(LogSoftmax())\n",
    "\n",
    "        self.children = network\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies all layers of neural network to the input.\n",
    "        Input shape:  [batch, num_units] (Variable)\n",
    "        Output shape: [batch, num_units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем функцию потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"Returns negative log-likelihood of target under model\n",
    "    represented by activations (log probabilities of classes).\n",
    "    Activations shape: [batch, num_classes] (Variable)\n",
    "    Target shape:      [batch]              (Variable)\n",
    "    Output shape: 1 (scalar, Variable)\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизатор SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        for param in self.parameters:\n",
    "            param.data -= self.learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(dataset, network, prefix='Test loss:', optimizer=None):\n",
    "    # Change mode for all layers.\n",
    "    if optimizer:\n",
    "        network.train()\n",
    "    else:\n",
    "        network.eval()\n",
    "\n",
    "    batch_size = 100\n",
    "    batchgenerator = torch.utils.data.DataLoader(dataset, batch_size, True)\n",
    "\n",
    "    avg_loss = 0\n",
    "    for i, (batch_data, batch_target) in enumerate(batchgenerator):\n",
    "        batch_output = network.forward(Variable(batch_data))\n",
    "        batch_loss = crossentropy(batch_output, Variable(batch_target))\n",
    "        batch_loss.backward()\n",
    "        batch_loss = batch_loss.data.numpy()[0]\n",
    "        avg_loss += (batch_loss - avg_loss) / (i + 1)\n",
    "        if optimizer:\n",
    "            optimizer.step()\n",
    "            network.zero_grad()\n",
    "    print(prefix, avg_loss, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.25001299381\n",
      "Test loss: 2.02255702019\n",
      "Train loss: 0.935892488275\n",
      "Test loss: 0.526892650127\n",
      "Train loss: 0.311101637781\n",
      "Test loss: 0.224462372065\n",
      "Train loss: 0.162839947534\n",
      "Test loss: 0.137114882469\n",
      "Train loss: 0.116679061204\n",
      "Test loss: 0.100092953444\n",
      "Train loss: 0.0960424696761\n",
      "Test loss: 0.0819583252072\n",
      "Train loss: 0.0743240392102\n",
      "Test loss: 0.0691537447274\n",
      "Train loss: 0.0632074608334\n",
      "Test loss: 0.0648413747549\n",
      "Train loss: 0.0556773274605\n",
      "Test loss: 0.0526380605996\n",
      "Train loss: 0.0497626392171\n",
      "Test loss: 0.0536358714104\n",
      "Train loss: 0.0436364341793\n",
      "Test loss: 0.0415103141218\n",
      "Train loss: 0.0387461567963\n",
      "Test loss: 0.0370218373835\n",
      "Train loss: 0.0330237374375\n",
      "Test loss: 0.0317714843899\n",
      "Train loss: 0.0313186772567\n",
      "Test loss: 0.0290575809777\n",
      "Train loss: 0.0266977072959\n",
      "Test loss: 0.0287078157067\n",
      "Train loss: 0.0241593799022\n",
      "Test loss: 0.0236087350175\n",
      "Train loss: 0.0228125809559\n",
      "Test loss: 0.0228643067181\n",
      "Train loss: 0.0213984650826\n",
      "Test loss: 0.0240131534636\n",
      "Train loss: 0.0181131877138\n",
      "Test loss: 0.0180235758424\n",
      "Train loss: 0.0175916215937\n",
      "Test loss: 0.0181138986722\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "sgd = SGDOptimizer(network.parameters(), 0.5)\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', sgd)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Больше оптимизаторов Б-гу Оптимизации!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGDMomentumOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, momentum=0.9):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        # your code here\n",
    "        \n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RMSPropOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, beta=0.9, eps=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        # your code here\n",
    "        \n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        # your code here\n",
    "        \n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.29899907112\n",
      "Test loss: 2.28840465546\n",
      "Train loss: 2.17900568247\n",
      "Test loss: 1.91997365952\n",
      "Train loss: 1.19421473571\n",
      "Test loss: 0.793489205837\n",
      "Train loss: 0.455302154379\n",
      "Test loss: 0.312181121111\n",
      "Train loss: 0.230511957513\n",
      "Test loss: 0.165832227468\n",
      "Train loss: 0.123873262533\n",
      "Test loss: 0.100607940555\n",
      "Train loss: 0.0892768991845\n",
      "Test loss: 0.0810534343123\n",
      "Train loss: 0.0676086062033\n",
      "Test loss: 0.0628551796079\n",
      "Train loss: 0.0543780475855\n",
      "Test loss: 0.0540404502302\n",
      "Train loss: 0.0464387893943\n",
      "Test loss: 0.0465289611369\n",
      "Train loss: 0.040847371798\n",
      "Test loss: 0.0428905472159\n",
      "Train loss: 0.037239708339\n",
      "Test loss: 0.0359449528158\n",
      "Train loss: 0.0327466359096\n",
      "Test loss: 0.0366176884621\n",
      "Train loss: 0.0296002742834\n",
      "Test loss: 0.0310783546418\n",
      "Train loss: 0.0265897470526\n",
      "Test loss: 0.0265863619745\n",
      "Train loss: 0.0248268215385\n",
      "Test loss: 0.0258533308282\n",
      "Train loss: 0.0228131134063\n",
      "Test loss: 0.0230030672625\n",
      "Train loss: 0.0207394263042\n",
      "Test loss: 0.0255728438497\n",
      "Train loss: 0.0193641397969\n",
      "Test loss: 0.0202927647159\n",
      "Train loss: 0.0178717284996\n",
      "Test loss: 0.0180073000491\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = SGDMomentumOptimizer(network.parameters(), 0.5)\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.04873784099\n",
      "Test loss: 0.38672837019\n",
      "Train loss: 0.227099137647\n",
      "Test loss: 0.188070484996\n",
      "Train loss: 0.132430793984\n",
      "Test loss: 0.121710123122\n",
      "Train loss: 0.0845916050353\n",
      "Test loss: 0.0908883780241\n",
      "Train loss: 0.0581328541573\n",
      "Test loss: 0.0690488003194\n",
      "Train loss: 0.0373754860567\n",
      "Test loss: 0.0434633551165\n",
      "Train loss: 0.0285121881004\n",
      "Test loss: 0.0349668972194\n",
      "Train loss: 0.0181794009903\n",
      "Test loss: 0.0249595091678\n",
      "Train loss: 0.0139850968761\n",
      "Test loss: 0.0204672420397\n",
      "Train loss: 0.0095160015127\n",
      "Test loss: 0.019333433453\n",
      "Train loss: 0.00756529153192\n",
      "Test loss: 0.0102690941188\n",
      "Train loss: 0.00454936275491\n",
      "Test loss: 0.0070749043487\n",
      "Train loss: 0.00344879847918\n",
      "Test loss: 0.00430776766734\n",
      "Train loss: 0.00373204343902\n",
      "Test loss: 0.00334942489862\n",
      "Train loss: 0.00152089097537\n",
      "Test loss: 0.00197805096395\n",
      "Train loss: 0.0013455981264\n",
      "Test loss: 0.00142132241745\n",
      "Train loss: 0.000820027600899\n",
      "Test loss: 0.000827933556866\n",
      "Train loss: 0.000583214284104\n",
      "Test loss: 0.000719902419951\n",
      "Train loss: 0.000449190720766\n",
      "Test loss: 0.000355705589755\n",
      "Train loss: 0.000906572375243\n",
      "Test loss: 0.000387026541284\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = RMSPropOptimizer(network.parameters())\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.984752461314\n",
      "Test loss: 0.311784574389\n",
      "Train loss: 0.334625060537\n",
      "Test loss: 0.129556299746\n",
      "Train loss: 0.147239101785\n",
      "Test loss: 0.13299726136\n",
      "Train loss: 0.0741345614993\n",
      "Test loss: 0.0770199432969\n",
      "Train loss: 0.0687923396805\n",
      "Test loss: 0.190458774939\n",
      "Train loss: 0.0397689110999\n",
      "Test loss: 0.0965252541006\n",
      "Train loss: 0.0178003121567\n",
      "Test loss: 0.040087666316\n",
      "Train loss: 0.0105867093745\n",
      "Test loss: 0.0139437015634\n",
      "Train loss: 0.00589249691049\n",
      "Test loss: 0.00134194488637\n",
      "Train loss: 0.00215421384616\n",
      "Test loss: 0.00138041300233\n",
      "Train loss: 0.00115795526654\n",
      "Test loss: 0.000901157624321\n",
      "Train loss: 0.000817922378441\n",
      "Test loss: 0.000593731965637\n",
      "Train loss: 0.00073160989684\n",
      "Test loss: 0.000453838970861\n",
      "Train loss: 0.000624272052456\n",
      "Test loss: 0.000451174081536\n",
      "Train loss: 0.000591980269486\n",
      "Test loss: 0.00039646657533\n",
      "Train loss: 0.000536981753872\n",
      "Test loss: 0.000367731397273\n",
      "Train loss: 0.000519900157607\n",
      "Test loss: 0.000337578472681\n",
      "Train loss: 0.000461065433878\n",
      "Test loss: 0.000309497537091\n",
      "Train loss: 0.000445698657651\n",
      "Test loss: 0.000284600986197\n",
      "Train loss: 0.000420782158991\n",
      "Test loss: 0.000307945412351\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = AdamOptimizer(network.parameters())\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты с DropOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот пункт обязателен к выполнению.\n",
    "Для того, чтобы получить бонусный балл за этот пункт, нужно эффективно реализовать DropOut:\n",
    "не вычислять активации выкинутых нейронов, прежде чем их обнулить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseWithDropOut(Module):\n",
    "    def __init__(self, input_units, output_units, dropout_rate, nonlinearity):\n",
    "        \"\"\"A dense layer is a layer which performs a learned\n",
    "        affine transformation and applies dropout:\n",
    "        m ~ Bernoulli(1 - p, size=output_units)\n",
    "        f(x) = g(W x + b) o m\n",
    "        \"\"\"\n",
    "        super(DenseWithDropOut, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nonlinearity = nonlinearity\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = None  # your code here\n",
    "        self.biases = None   # your code here\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Performs an affine transformation with dropout.\n",
    "        In training mode:\n",
    "        m ~ Bernoulli(1 - p, size=output_units)\n",
    "        f(x) = g(W x + b) o m\n",
    "        In evaluation mode:\n",
    "        f(x) = g(W x + b) (1 - p)\n",
    "        input shape:  [batch, input_units]  (Variable)\n",
    "        output shape: [batch, output units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, верно ли, что полносвязная сеть с dropout работает быстрее, чем обычная полносвязная сеть, поскольку на каждом проходе вычисляются произведения матриц меньшего размера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54 s ± 191 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "7.3 s ± 954 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "width = 2000\n",
    "network1 = [\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x)),\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x)),\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x)),\n",
    "    DenseWithDropOut(width, 1, 0, lambda x: x)\n",
    "]\n",
    "network2 = [\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, 1)\n",
    "]\n",
    "X = torch.randn(10000, width)\n",
    "\n",
    "# check whether DenseWithDropOut works faster than Dense\n",
    "def test_network(network):\n",
    "    x = Variable(X)\n",
    "    for layer in network:\n",
    "        x = layer.forward(x)\n",
    "    x.mean().backward()\n",
    "    for layer in network:\n",
    "        x = layer.zero_grad()\n",
    "\n",
    "test_network(network1)\n",
    "%timeit test_network(network1)\n",
    "%timeit test_network(network2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для более узких слоев, меньших dropout rate и меньших размеров батча увеличение производительности не настолько существенно или может вообще отсутствовать."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
