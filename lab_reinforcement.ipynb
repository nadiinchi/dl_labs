{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|-|-|\n",
    "| ![gym](images/gym.png) | ![img](images/pytorch.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание\n",
    "\n",
    "Является взаимодополняющим с заданием по контекстным бандитам, в сумме достаточно набрать 10 баллов.\n",
    "\n",
    "1. [5 баллов] Для произвольно выбранной игры из набора Atari реализовать нейросетевого агента для среды из регистров памяти.\n",
    "\n",
    "2. [5 баллов] Реализовать агента для среды со снимками экрана и сравнить с первым агентом.\n",
    "\n",
    "Приведите логи изменения продолжительности игры или количества набранных очков.\n",
    "\n",
    "Приложите ссылку на запись с моментами из игры обученного агента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The typical imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом занятии будем использовать библиотеку [openai](https://openai.com/systems/) `gym`, `pytorch` также будет использоваться для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install pyglet==1.2.4\n",
    "# !pip install gym==0.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Агент взаимодействует со средой через действия (**A**), изменяя свое состояние (**S**) и получая вознаграждение (**R**).\n",
    "\n",
    "Итоговая цель -- максимизировать суммарное вознаграждение.\n",
    "\n",
    "![](images/recap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тележка\n",
    "\n",
    "![](images/cartpole.png)\n",
    "\n",
    "* **Цель** - как можно дольше удерживать стержень вертикально\n",
    "* **Состояние** - угол, угловая скорость, положение, горизонтальная скорость\n",
    "* **Действие** - горизонтальная сила, применяемая к тележке\n",
    "* **Вознаграждение** - 1 за каждый момент времени с околовертикальным стержнем (например, 85-95 градусов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari\n",
    "\n",
    "![](images/atari.png)\n",
    "\n",
    "* **Цель** - побить рекорд по очкам\n",
    "* **Состояние** - экран игры (изображение)\n",
    "* **Действие** - различные клавиши\n",
    "* **Вознаграждение** - определяется игрой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doom\n",
    "\n",
    "![](images/doom.png)\n",
    "\n",
    "* **Цель** - уничтожить всех противников\n",
    "* **Состояние** - экран игры (изображение)\n",
    "* **Действие** - различные клавиши\n",
    "* **Вознаграждение** - +1 при убийстве противника, -N при смерти героя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дисконтирование вознаграждений\n",
    "\n",
    "В задачах обучения с подкреплениям часто вводится дисконтирующий коэффицент $\\gamma$.\n",
    "\n",
    "Без него суммарное вознаграждение за все будущие состояние с момента $t$ может быть определено как\n",
    "\n",
    "![](images/nodiscount.png)\n",
    "\n",
    "С ним же мы отдаем предпочтения больше текущим вознаграждениям, чем будущим, т.к. среда может быть изменчива:\n",
    "\n",
    "![](images/discount.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция ценности\n",
    "\n",
    "Хочется для каждого состояния знать, насколько оно \"хорошее\" (valuable). Тогда мы бы знали, чего ожидать от перехода в данное состояние.\n",
    "\n",
    "Для этого и существуют функции ценности -- они вычисляют средний дисконтированный выигрыш при следовании некоторой стратегии $\\pi$.\n",
    "\n",
    "![](images/vpolicy.png)\n",
    "\n",
    "Существует также и функция, определяющая оптимальную оценку состояния как максимум оценок среди всех стратегий.\n",
    "\n",
    "![](images/voptimal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-функция\n",
    "\n",
    "Даже при наличии оптимальных оценок состояний, мы все еще не можем их произвольно устанавливать, а только выбирать действие `a`.\n",
    "\n",
    "Для эффективного выбора действия при некотором состояния есть еще Q-функция (оценивающая Quality$\\approx$эффективность такого действия).\n",
    "\n",
    "Более формально -- $Q^\\pi(s, a)$ выражает ожидаемое вознаграждение при выполнении действия `a` и дальнейшем следовании стратегии $\\pi$.\n",
    "\n",
    "Также как и с оценкой состоянии существует некоторая оптимальная $Q^*(s, a)$.\n",
    "\n",
    "Между двумя оптимальными функциями можно установить тождество:\n",
    "\n",
    "![](images/VQ.png)\n",
    "\n",
    "А оптимальная стратегия в свою очередь выражается через оптимальную Q-функцию:\n",
    "\n",
    "![](images/pioptimal.png)\n",
    "\n",
    "**Задача** поиска оптимальной стратегии для агента в итоге **сводится к определению $V^*$ и $Q^*$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Q(s, a) можно определить рекурсивно через уравнение Беллмана как\n",
    "\n",
    "![](images/bellman.png)\n",
    "\n",
    "Идея Q-обучения состоит в том, чтобы исходя из уравнения Беллмана оценивать Q итеративно:\n",
    "\n",
    "![](images/qiter.png)\n",
    "\n",
    "Изначально это ничего не дает, т.к. базовые аппроксимации будут случайными. Но чем больше узнает агент, тем ближе мы к $Q^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Самодельная\" среда\n",
    "\n",
    "![](images/zombie.png)\n",
    "\n",
    "Хотим съесть мороженое, и не натыкаться на зомби, действия -- выбор одной из 4 сторон для перемещения.\n",
    "\n",
    "Начальное состояние среды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i *\n",
      "z c\n"
     ]
    }
   ],
   "source": [
    "ZOMBIE = \"z\"\n",
    "CAR = \"c\"\n",
    "ICE_CREAM = \"i\"\n",
    "EMPTY = \"*\"\n",
    "\n",
    "grid = [\n",
    "    [ICE_CREAM, EMPTY],\n",
    "    [ZOMBIE, CAR]\n",
    "]\n",
    "\n",
    "for row in grid:\n",
    "    print(' '.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс состояний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, grid, car_pos):\n",
    "        self.grid = grid\n",
    "        self.car_pos = car_pos\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, State) and self.grid == other.grid and self.car_pos == other.car_pos\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self.grid) + str(self.car_pos))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"State(grid={self.grid}, car_pos={self.car_pos})\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И опять же начальное состояние"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(grid=[['i', '*'], ['z', 'c']], car_pos=[1, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_state = State(grid=grid, car_pos=[1, 1])\n",
    "start_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция взаимодействия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def act(state, action):\n",
    "    def new_car_pos(state, action):\n",
    "        p = deepcopy(state.car_pos)\n",
    "        if action == UP:\n",
    "            p[0] = max(0, p[0] - 1)\n",
    "        elif action == DOWN:\n",
    "            p[0] = min(len(state.grid) - 1, p[0] + 1)\n",
    "        elif action == LEFT:\n",
    "            p[1] = max(0, p[1] - 1)\n",
    "        elif action == RIGHT:\n",
    "            p[1] = min(len(state.grid[0]) - 1, p[1] + 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown action {action}\")\n",
    "        return p\n",
    "\n",
    "    p = new_car_pos(state, action)\n",
    "    grid_item = state.grid[p[0]][p[1]]\n",
    "    \n",
    "    new_grid = deepcopy(state.grid)\n",
    "    \n",
    "    if grid_item == ZOMBIE:\n",
    "        reward = -100\n",
    "        is_done = True\n",
    "        new_grid[p[0]][p[1]] += CAR\n",
    "    elif grid_item == ICE_CREAM:\n",
    "        reward = 1000\n",
    "        is_done = True\n",
    "        new_grid[p[0]][p[1]] += CAR\n",
    "    elif grid_item == EMPTY:\n",
    "        reward = -1\n",
    "        is_done = False\n",
    "        old = state.car_pos\n",
    "        new_grid[old[0]][old[1]] = EMPTY\n",
    "        new_grid[p[0]][p[1]] = CAR\n",
    "    elif grid_item == CAR:\n",
    "        reward = -1\n",
    "        is_done = False\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown grid item {grid_item}\")\n",
    "\n",
    "    return State(grid=new_grid, car_pos=p), reward, is_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "N_STATES = 4\n",
    "N_EPISODES = 20\n",
    "\n",
    "MAX_EPISODE_STEPS = 100\n",
    "\n",
    "MIN_ALPHA = 0.02\n",
    "\n",
    "alphas = np.linspace(1.0, MIN_ALPHA, N_EPISODES)\n",
    "gamma = 1.0\n",
    "eps = 0.2\n",
    "\n",
    "q_table = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q(state, action=None):\n",
    "    if state not in q_table:\n",
    "        q_table[state] = np.zeros(len(ACTIONS))\n",
    "        \n",
    "    if action is None:\n",
    "        return q_table[state]\n",
    "    \n",
    "    return q_table[state][action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < eps:\n",
    "        return random.choice(ACTIONS) \n",
    "    else:\n",
    "        return np.argmax(q(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: total reward -> 999\n",
      "Episode 2: total reward -> 998\n",
      "Episode 3: total reward -> 997\n",
      "Episode 4: total reward -> 997\n",
      "Episode 5: total reward -> 999\n",
      "Episode 6: total reward -> 999\n",
      "Episode 7: total reward -> 998\n",
      "Episode 8: total reward -> -100\n",
      "Episode 9: total reward -> -101\n",
      "Episode 10: total reward -> 999\n",
      "Episode 11: total reward -> 999\n",
      "Episode 12: total reward -> 999\n",
      "Episode 13: total reward -> 999\n",
      "Episode 14: total reward -> 999\n",
      "Episode 15: total reward -> 999\n",
      "Episode 16: total reward -> 998\n",
      "Episode 17: total reward -> 999\n",
      "Episode 18: total reward -> 999\n",
      "Episode 19: total reward -> 999\n",
      "Episode 20: total reward -> 999\n"
     ]
    }
   ],
   "source": [
    "for e in range(N_EPISODES):\n",
    "    state = start_state\n",
    "    total_reward = 0\n",
    "    alpha = alphas[e]\n",
    "    \n",
    "    for _ in range(MAX_EPISODE_STEPS):\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done = act(state, action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        q(state)[action] = q(state, action) + \\\n",
    "                alpha * (reward + gamma *  np.max(q(next_state)) - q(state, action))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    print(f\"Episode {e + 1}: total reward -> {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{State(grid=[['i', '*'], ['z', 'c']], car_pos=[1, 1]): array([998.9999565 , 225.12936017, -85.10182825, 586.19245204]),\n",
       " State(grid=[['i', '*'], ['zc', 'c']], car_pos=[1, 0]): array([0., 0., 0., 0.]),\n",
       " State(grid=[['i', 'c'], ['z', '*']], car_pos=[0, 1]): array([ 895.94526316,  842.8767095 , 1000.        ,  967.10727091]),\n",
       " State(grid=[['ic', 'c'], ['z', '*']], car_pos=[0, 0]): array([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(grid=[['i', '*'], ['z', 'c']], car_pos=[1, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(grid=[['i', 'c'], ['z', '*']], car_pos=[0, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_1 = act(start_state, choose_action(start_state))[0]\n",
    "state_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(grid=[['i', '*'], ['z', 'c']], car_pos=[1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_2 = act(state_1, choose_action(state_1))[0]\n",
    "state_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openai Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-18 20:32:59,691] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for t in range(5000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08355633,  0.55682708, -0.21051092, -1.24616204])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def demo(name):\n",
    "    env = gym.make(name)\n",
    "    try:\n",
    "        for i_episode in range(5):\n",
    "            observation = env.reset()\n",
    "            for t in range(200):\n",
    "                env.render()\n",
    "                action = env.action_space.sample()\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                if done:\n",
    "                    print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                    env.close()\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-18 20:35:30,817] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "demo('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.80000000e+00 3.40282347e+38 4.18879020e-01 3.40282347e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.80000000e+00 -3.40282347e+38 -4.18879020e-01 -3.40282347e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "space = spaces.Discrete(8) # Set with 8 elements {0, 1, 2, ..., 7}\n",
    "x = space.sample()\n",
    "assert space.contains(x)\n",
    "assert space.n == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-18 20:37:55,306] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    }
   ],
   "source": [
    "demo('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-18 20:39:10,083] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 166 timesteps\n",
      "Episode finished after 167 timesteps\n",
      "Episode finished after 168 timesteps\n",
      "Episode finished after 169 timesteps\n",
      "Episode finished after 170 timesteps\n",
      "Episode finished after 171 timesteps\n",
      "Episode finished after 172 timesteps\n",
      "Episode finished after 173 timesteps\n",
      "Episode finished after 174 timesteps\n",
      "Episode finished after 175 timesteps\n",
      "Episode finished after 176 timesteps\n",
      "Episode finished after 177 timesteps\n",
      "Episode finished after 178 timesteps\n",
      "Episode finished after 179 timesteps\n",
      "Episode finished after 180 timesteps\n",
      "Episode finished after 181 timesteps\n",
      "Episode finished after 182 timesteps\n",
      "Episode finished after 183 timesteps\n",
      "Episode finished after 184 timesteps\n",
      "Episode finished after 185 timesteps\n",
      "Episode finished after 186 timesteps\n",
      "Episode finished after 187 timesteps\n",
      "Episode finished after 188 timesteps\n",
      "Episode finished after 189 timesteps\n",
      "Episode finished after 190 timesteps\n",
      "Episode finished after 191 timesteps\n",
      "Episode finished after 192 timesteps\n",
      "Episode finished after 193 timesteps\n",
      "Episode finished after 194 timesteps\n",
      "Episode finished after 195 timesteps\n",
      "Episode finished after 196 timesteps\n",
      "Episode finished after 197 timesteps\n",
      "Episode finished after 198 timesteps\n",
      "Episode finished after 199 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 170 timesteps\n",
      "Episode finished after 171 timesteps\n",
      "Episode finished after 172 timesteps\n",
      "Episode finished after 173 timesteps\n",
      "Episode finished after 174 timesteps\n",
      "Episode finished after 175 timesteps\n",
      "Episode finished after 176 timesteps\n",
      "Episode finished after 177 timesteps\n",
      "Episode finished after 178 timesteps\n",
      "Episode finished after 179 timesteps\n",
      "Episode finished after 180 timesteps\n",
      "Episode finished after 181 timesteps\n",
      "Episode finished after 182 timesteps\n",
      "Episode finished after 183 timesteps\n",
      "Episode finished after 184 timesteps\n",
      "Episode finished after 185 timesteps\n",
      "Episode finished after 186 timesteps\n",
      "Episode finished after 187 timesteps\n",
      "Episode finished after 188 timesteps\n",
      "Episode finished after 189 timesteps\n",
      "Episode finished after 190 timesteps\n",
      "Episode finished after 191 timesteps\n",
      "Episode finished after 192 timesteps\n",
      "Episode finished after 193 timesteps\n",
      "Episode finished after 194 timesteps\n",
      "Episode finished after 195 timesteps\n",
      "Episode finished after 196 timesteps\n",
      "Episode finished after 197 timesteps\n",
      "Episode finished after 198 timesteps\n",
      "Episode finished after 199 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 193 timesteps\n",
      "Episode finished after 194 timesteps\n",
      "Episode finished after 195 timesteps\n",
      "Episode finished after 196 timesteps\n",
      "Episode finished after 197 timesteps\n",
      "Episode finished after 198 timesteps\n",
      "Episode finished after 199 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    }
   ],
   "source": [
    "demo('Breakout-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-18 20:45:18,723] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1276923c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).type(Tensor)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN()\n",
    "target_net = DQN()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    policy_net.cuda()\n",
    "    target_net.cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return policy_net(\n",
    "            Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]])\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)))\n",
    "    non_final_next_states = Variable(torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]),\n",
    "                                     volatile=True)\n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor))\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    # Undo volatility (which was used to prevent unnecessary gradients)\n",
    "    expected_state_action_values = Variable(expected_state_action_values.data)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129107978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129107978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action[0, 0])\n",
    "        reward = Tensor([reward])\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
