{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar on recurrent neural networks\n",
    "During this seminar, we will train LSTM to solve sentiment analysis task, i. e. predict sentiment label for the text.\n",
    "\n",
    "The recurrent neural networks work with the input of arbitrary length. However, in the implementation, it is usually much simpler to fix the sequence length (even in pytorch with its dynamic graphs :) So we will crop the sequences so that they have fixed length.\n",
    "\n",
    "During completing this task, you will train LSTM with different level of \"black box\" (from using torch.nn.LSTM to implementing layer yourself). Also, you will learn different ways of applying dropout to the gated RNNs (in the RNNs there are more places where to insert binary dropout mask than in feed-forward networks).\n",
    "\n",
    "The task can be completed on CPU but you will feel more comfortable with GPU. You may try using [https://colab.research.google.com](https://colab.research.google.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000 \n",
    "index_from = 3\n",
    "n_hidden = 32 # 128\n",
    "n_emb = 32 # 128\n",
    "seq_len = 32 # 200\n",
    "# small network on small data for seminar purposes\n",
    "# after # normal size goes\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "Function load_matrix_imdb downloads data, preprocesses it and returns numpy-arrays. \n",
    "\n",
    "If you don't have wget, please download [archive imdb.npz](https://s3.amazonaws.com/text-datasets/imdb.npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rnn_utils import load_matrix_imdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrix_imdb(path='imdb.npz', num_words=None, skip_top=0,\n",
    "              maxlen=None, seed=113,\n",
    "              start_char=1, oov_char=2, index_from=3, **kwargs):\n",
    "    \"\"\"\n",
    "    Modified code from Keras\n",
    "    Loads data matrixes from npz file, crops and pads seqs and returns\n",
    "    shuffled (x_train, y_train), (x_test, y_test)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Downloading matrix data into current folder\")\n",
    "        os.system(\"wget https://s3.amazonaws.com/text-datasets/imdb.npz\")\n",
    "        \n",
    "    with np.load(path, allow_pickle=True) as f:\n",
    "        x_train, labels_train = f['x_train'], f['y_train']\n",
    "        x_test, labels_test = f['x_test'], f['y_test']\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(x_train))\n",
    "    np.random.shuffle(indices)\n",
    "    x_train = x_train[indices]\n",
    "    labels_train = labels_train[indices]\n",
    "\n",
    "    indices = np.arange(len(x_test))\n",
    "    np.random.shuffle(indices)\n",
    "    x_test = x_test[indices]\n",
    "    labels_test = labels_test[indices]\n",
    "\n",
    "    xs = np.concatenate([x_train, x_test])\n",
    "    labels = np.concatenate([labels_train, labels_test])\n",
    "\n",
    "    if start_char is not None:\n",
    "        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n",
    "    elif index_from:\n",
    "        xs = [[w + index_from for w in x] for x in xs]\n",
    "\n",
    "    if not num_words:\n",
    "        num_words = max([max(x) for x in xs])\n",
    "    if not maxlen:\n",
    "        maxlen = max([len(x) for x in xs])\n",
    "\n",
    "    # by convention, use 2 as OOV word\n",
    "    # reserve 'index_from' (=3 by default) characters:\n",
    "    # 0 (padding), 1 (start), 2 (OOV)\n",
    "    xs_new = []\n",
    "    for x in xs:\n",
    "        x = x[:maxlen] # crop long sequences\n",
    "        if oov_char is not None: # replace rare or frequent symbols \n",
    "            x = [w if (skip_top <= w < num_words) else oov_char for w in x]\n",
    "        else: # or filter rare and frequent symbols\n",
    "            x = [w for w in x if skip_top <= w < num_words]\n",
    "        x_padded = np.zeros(maxlen)#, dtype = 'int32')\n",
    "        x_padded[-len(x):] = x\n",
    "        xs_new.append(x_padded)    \n",
    "            \n",
    "    idx = len(x_train)\n",
    "    x_train, y_train = np.array(xs_new[:idx]), np.array(labels[:idx])\n",
    "    x_test, y_test = np.array(xs_new[idx:]), np.array(labels[idx:])\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "(X_train, y_train), (X_test, y_test) = load_matrix_imdb(num_words=vocab_size,\n",
    "                                                        maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train) # binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 32), (25000, 32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.000e+00, 1.400e+01, 2.200e+01, 1.600e+01, 4.300e+01, 5.300e+02,\n",
       "       9.730e+02, 1.622e+03, 1.385e+03, 6.500e+01, 4.580e+02, 4.468e+03,\n",
       "       6.600e+01, 3.941e+03, 4.000e+00, 1.730e+02, 3.600e+01, 2.560e+02,\n",
       "       5.000e+00, 2.500e+01, 1.000e+02, 4.300e+01, 8.380e+02, 1.120e+02,\n",
       "       5.000e+01, 6.700e+02, 2.000e+00, 9.000e+00, 3.500e+01, 4.800e+02,\n",
       "       2.840e+02, 5.000e+00])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] # sequence of coded words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.long), \n",
    "                               torch.tensor(y_train, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long), \n",
    "                               torch.tensor(y_test, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and training RNN in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RNN will process the input sequence by words (word level). We will use simple architecture consisting of embedding layer, 1 LSTM layer and fully-connected layer on the last hidden state.\n",
    "\n",
    "The code below defines and trains the network. __Pay attention__ to \"### pay attention here\" marks: they point to RNN specifics.\n",
    "\n",
    "Run this code so that you can compare training time with different models and implementations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, \\\n",
    "                 batch_size, rec_layer=nn.LSTM, embedding=nn.Embedding, \\\n",
    "                 dropout=None):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.word_embeddings = embedding(vocab_size, embedding_dim)\n",
    "        if dropout:\n",
    "            self.rnn = rec_layer(embedding_dim, hidden_dim, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = rec_layer(embedding_dim, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        embedding = self.word_embeddings(sentences)\n",
    "        out, hidden = self.rnn(embedding) # pay attention here!\n",
    "        res = self.hidden2label(out[-1])\n",
    "        return torch.sigmoid(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LSTM source code](http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNNClassifier(embedding_dim=n_emb,\n",
    "                       hidden_dim=n_hidden,\n",
    "                       vocab_size=vocab_size,\n",
    "                       label_size=1,\n",
    "                       batch_size=batch_size, \n",
    "                       rec_layer=nn.LSTM,\n",
    "                       dropout=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "lossfun = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, model, lossfun, optimizer, device):\n",
    "    model.train()\n",
    "    for it, traindata in enumerate(train_loader):\n",
    "        train_inputs, train_labels = traindata\n",
    "        train_inputs = train_inputs.to(device) \n",
    "        train_labels = train_labels.to(device)\n",
    "        train_labels = torch.squeeze(train_labels)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        output = model(train_inputs.t()) # pay attention here!\n",
    "\n",
    "        loss = lossfun(output.view(-1), train_labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(loader, model, lossfun, device):\n",
    "    model.eval()\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total = 0.0\n",
    "    for it, data in enumerate(loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "        labels = torch.squeeze(labels)\n",
    "\n",
    "        output = model(inputs.t()) # pay attention here!\n",
    "        loss = lossfun(output.view(-1), labels.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # calc testing acc        \n",
    "        pred = output.view(-1) > 0.5\n",
    "        correct = pred == labels.bool()\n",
    "        total_acc += torch.sum(correct).item() / len(correct)\n",
    "\n",
    "    total = it + 1\n",
    "    return total_loss / total, total_acc / total\n",
    "    \n",
    "\n",
    "def train(train_loader, test_loader, model, lossfun, optimizer, \\\n",
    "          device, num_epochs):\n",
    "    train_loss_ = []\n",
    "    test_loss_ = []\n",
    "    train_acc_ = []\n",
    "    test_acc_ = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(train_loader, model, lossfun, optimizer, device)\n",
    "        train_loss, train_acc = evaluate(train_loader, model, lossfun, device)\n",
    "        train_loss_.append(train_loss)\n",
    "        train_acc_.append(train_acc)\n",
    "        test_loss, test_acc = evaluate(test_loader, model, lossfun, device)\n",
    "        test_loss_.append(test_loss)\n",
    "        test_acc_.append(test_acc)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:3d}/{num_epochs:3d} '\n",
    "              f'Training Loss: {train_loss_[epoch]:.3f}, Testing Loss: {test_loss_[epoch]:.3f}, '\n",
    "              f'Training Acc: {train_acc_[epoch]:.3f}, Testing Acc: {test_acc_[epoch]:.3f}')\n",
    "\n",
    "    return train_loss_, train_acc_, test_loss_, test_acc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/ 30 Training Loss: 84.081, Testing Loss: 85.471, Training Acc: 0.610, Testing Acc: 0.591\n",
      "Epoch:   2/ 30 Training Loss: 75.500, Testing Loss: 80.116, Training Acc: 0.689, Testing Acc: 0.647\n",
      "Epoch:   3/ 30 Training Loss: 69.199, Testing Loss: 77.381, Training Acc: 0.727, Testing Acc: 0.672\n",
      "Epoch:   4/ 30 Training Loss: 66.386, Testing Loss: 76.913, Training Acc: 0.742, Testing Acc: 0.681\n",
      "Epoch:   5/ 30 Training Loss: 62.392, Testing Loss: 76.142, Training Acc: 0.761, Testing Acc: 0.682\n",
      "Epoch:   6/ 30 Training Loss: 56.279, Testing Loss: 74.291, Training Acc: 0.797, Testing Acc: 0.699\n",
      "Epoch:   7/ 30 Training Loss: 53.556, Testing Loss: 74.636, Training Acc: 0.808, Testing Acc: 0.706\n",
      "Epoch:   8/ 30 Training Loss: 50.282, Testing Loss: 73.452, Training Acc: 0.828, Testing Acc: 0.705\n",
      "Epoch:   9/ 30 Training Loss: 46.989, Testing Loss: 78.924, Training Acc: 0.837, Testing Acc: 0.704\n",
      "Epoch:  10/ 30 Training Loss: 43.098, Testing Loss: 78.249, Training Acc: 0.858, Testing Acc: 0.711\n",
      "Epoch:  11/ 30 Training Loss: 41.002, Testing Loss: 82.262, Training Acc: 0.863, Testing Acc: 0.714\n",
      "Epoch:  12/ 30 Training Loss: 37.565, Testing Loss: 81.205, Training Acc: 0.879, Testing Acc: 0.709\n",
      "Epoch:  13/ 30 Training Loss: 34.610, Testing Loss: 87.018, Training Acc: 0.890, Testing Acc: 0.711\n",
      "Epoch:  14/ 30 Training Loss: 32.602, Testing Loss: 92.635, Training Acc: 0.897, Testing Acc: 0.706\n",
      "Epoch:  15/ 30 Training Loss: 29.850, Testing Loss: 90.952, Training Acc: 0.909, Testing Acc: 0.708\n",
      "Epoch:  16/ 30 Training Loss: 28.199, Testing Loss: 101.265, Training Acc: 0.913, Testing Acc: 0.703\n",
      "Epoch:  17/ 30 Training Loss: 25.259, Testing Loss: 99.151, Training Acc: 0.926, Testing Acc: 0.705\n",
      "Epoch:  18/ 30 Training Loss: 23.200, Testing Loss: 101.847, Training Acc: 0.935, Testing Acc: 0.703\n",
      "Epoch:  19/ 30 Training Loss: 21.862, Testing Loss: 114.806, Training Acc: 0.937, Testing Acc: 0.700\n",
      "Epoch:  20/ 30 Training Loss: 19.001, Testing Loss: 125.061, Training Acc: 0.946, Testing Acc: 0.702\n",
      "Epoch:  21/ 30 Training Loss: 18.622, Testing Loss: 116.497, Training Acc: 0.947, Testing Acc: 0.703\n",
      "Epoch:  22/ 30 Training Loss: 15.957, Testing Loss: 121.591, Training Acc: 0.959, Testing Acc: 0.699\n",
      "Epoch:  23/ 30 Training Loss: 13.940, Testing Loss: 137.616, Training Acc: 0.964, Testing Acc: 0.700\n",
      "Epoch:  24/ 30 Training Loss: 12.576, Testing Loss: 139.046, Training Acc: 0.968, Testing Acc: 0.699\n",
      "Epoch:  25/ 30 Training Loss: 14.242, Testing Loss: 130.667, Training Acc: 0.964, Testing Acc: 0.693\n",
      "Epoch:  26/ 30 Training Loss: 9.967, Testing Loss: 165.656, Training Acc: 0.975, Testing Acc: 0.694\n",
      "Epoch:  27/ 30 Training Loss: 9.035, Testing Loss: 157.930, Training Acc: 0.980, Testing Acc: 0.697\n",
      "Epoch:  28/ 30 Training Loss: 7.417, Testing Loss: 169.353, Training Acc: 0.984, Testing Acc: 0.695\n",
      "Epoch:  29/ 30 Training Loss: 6.912, Testing Loss: 180.652, Training Acc: 0.985, Testing Acc: 0.692\n",
      "Epoch:  30/ 30 Training Loss: 5.785, Testing Loss: 191.858, Training Acc: 0.988, Testing Acc: 0.691\n",
      "CPU times: user 21min 2s, sys: 42.8 s, total: 21min 45s\n",
      "Wall time: 8min 39s\n"
     ]
    }
   ],
   "source": [
    "%time a, b, c, d = train(train_loader, test_loader, model, lossfun, \\\n",
    "                   optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unregularized LSTM often overfits (and we see that test accuracy degrates). To overcome it, L2 regularization and droput are usually applied. But there are several ways how to apply dropout to gated RNNs, and not all of them work well. Please refer to this [blog post](https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b) for good review.\n",
    "\n",
    "We will implement three dropouts for LSTM. While doing so, we will see that for different methods we need to \"reveal\" the layer to the different \"depth\" (from black-box to implementing layer ourselves)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout by (Gal and Ghahramani)\n",
    "\n",
    "Let's start with a dropout proposed by [Gal and Ghahramani](https://arxiv.org/abs/1512.05287).\n",
    "\n",
    "To implement it, we have to use nn.LSTMCell (processes 1 time step) instead of nn.LSTM (processes the whole input sequence). \n",
    "\n",
    "Complete class RNNLayer. With dropout=0 it has to work as usual LSTM, and with dropout > 0 it has to multiply the input and hidden vector by random binary mask, and this mask should be __the same for all time steps__.\n",
    "\n",
    "Formulas for this dropout (m denotes applying dropout):\n",
    "$$\n",
    "h_{t-1} = h_{t-1} \\odot m_h, \\, x_t = x_t \\odot m_x\n",
    "$$\n",
    "(after this, usual LSTM step goes)\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
    "h_t =  o \\odot tanh(c_t) \\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_h0_c0(num_objects, hidden_size, some_existing_tensor):\n",
    "    \"\"\"\n",
    "    return h0 and c0, use some_existing_tensor.new_zeros() to gen them\n",
    "    h0 shape: num_objects x hidden_size\n",
    "    c0 shape: num_objects x hidden_size\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_dropout_mask(input_size, hidden_size, is_training, p, some_existing_tensor):\n",
    "    \"\"\"\n",
    "    is_training: if True, gen masks from Bernoulli\n",
    "                 if False, gen masks consisting of (1-p)\n",
    "    \n",
    "    return two dropout masks of sizes (input_size, ), (hidden_size, )\n",
    "    if p is not None\n",
    "    return one masks if p is None\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    ...new_zeros(...).bernoulli(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=None):\n",
    "        super(RNNLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.dropout = dropout\n",
    "        self.rnn_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # initialize h_0, c_0\n",
    "        h_0, c_0 = init_h0_c0(inp.shape[1], self.hidden_size, inp)\n",
    "        \n",
    "        # gen masks\n",
    "        input_mask, hidden_mask = gen_dropout_mask(self.input_size, \\\n",
    "                                                   self.hidden_size, \\\n",
    "                                                   self.training, \\\n",
    "                                                   self.dropout, \\\n",
    "                                                   inp)\n",
    "        \n",
    "        \n",
    "        ### your code here\n",
    "        ### implement recurrent logic and return what nn.LSTM returns\n",
    "        ### do not forget to apply generated dropout masks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation with dropout turned off (pass RNNLayer to RNNClassifier as rec_layer). Measure the training time (%time). Does it differ from training time of nn.LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation with dropout=0.5 again measuring the training time. Does the model still overfit? Does the training take more time than training without dropout? (additional time is spent for mask generating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout by (Gal and Ghahramani). Second try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< start hacking pytorch >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you unroll the time  cycle in python, training slows down. But there is the way how to implement dropout by (Gal and Ghahramani) without modifying computational graph and modifying only weights of the network. This allows using nn.LSTM instead of nn.LSTMCell. Before calling nn.LSTM, you should replace its weights with the weights where some rows are multiplied by 0. Of course, in this case you have to store the trainable weights separately. This is the way how this dropout is implementd in FastAI library, which code  is used in the cell below.\n",
    "\n",
    "Complete the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FastRNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0):\n",
    "        super(FastRNNLayer, self).__init__()\n",
    "        self.module = nn.LSTM(input_size, hidden_size)\n",
    "        self.dropout = dropout\n",
    "        self.layer_names = ['weight_hh_l0', 'weight_ih_l0']\n",
    "        for layer in self.layer_names:\n",
    "            # Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            \n",
    "    def _setweights(self):\n",
    "        \"Apply dropout to the raw weights.\"\n",
    "        ### your code here\n",
    "        ### generate input_mask and hidden_mask (use function gen_dropout_mask)\n",
    "        \n",
    "        for layer, mask in zip(self.layer_names, (hidden_mask, input_mask)):\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = raw_w * mask\n",
    "\n",
    "    def forward(self, *args):\n",
    "        with warnings.catch_warnings():\n",
    "            # To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            ### your code here\n",
    "            ### set new weights of self.module and call its forward\n",
    "\n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation (again passing FastRNNLayer as a rec_layer) with dropout = 0.5. Compare training time with previous models. The test accuracy and other training metrics should be the same as with previous implementaton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< end hacking pytorch >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout by (Semeniuta et al)\n",
    "Now let's turn to implementing dropout proposed by [Semeniuta et al](http://www.aclweb.org/anthology/C16-1165). \n",
    "\n",
    "This method is even more popular than the previous one. It is developed specifically for _gated_ recurrent architectures. For LSTM, this dropout applies dropout only to information flow ($m_h$ is a dropout mask):\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot g \\odot {\\bf m_h} \\quad\n",
    "h_t =  o \\odot tanh(c_t) \\nonumber\n",
    "$$\n",
    "For $x_t$, the mask is put in the same way as in (Gal and Ghahramani). By the way, you can apply this mask before passing tensor to LSTM layer.\n",
    "\n",
    "According to the paper, the mask can be the same for all moments of the time but may also be  different. We will use the same mask.\n",
    "\n",
    "To implement this dropout, you have to implement LSTM by yourself (interface of LSTMCell is not enough as you should work with LSTM logics). Complete the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HandmadeLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.):\n",
    "        super(HandmadeLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.input_weights = nn.Linear(input_size, 4 * hidden_size)\n",
    "        self.hidden_weights = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "        \n",
    "        self.reset_params()\n",
    "\n",
    "\n",
    "    def reset_params(self):\n",
    "        \"\"\"\n",
    "        initialization as in Pytorch\n",
    "        do not forget to call this method!\n",
    "        \"\"\"\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(0, stdv)\n",
    "            \n",
    "\n",
    "    def forward(self, inp, hidden=None):\n",
    "        ### your code here\n",
    "        # use functions init_h0_c0 and gen_dropout_masks defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation without dropout (controlthe quality and compare the training time with nn.LSTM) and with dropout=0.5. Copare the quality with the model trained with the previous dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Zoneout\n",
    "In Zoneout, at each time step you update the hidden state with probability p and hold it the same with probability 1-p. Formulas for Zoneout:\n",
    " \n",
    "(firstly usual time step goes, e. g. LSTM)\n",
    "$$\n",
    "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
    "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
    "$$\n",
    "$$\n",
    "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
    "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
    "$$\n",
    "$$\n",
    "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
    "h_t =  o \\odot tanh(c_t) \\nonumber\n",
    "$$\n",
    "Then apply Zoneout:\n",
    "$$\n",
    "h_t = h_t \\odot m_h^t + h_{t-1}\\odot(1-m_h^t)\n",
    "$$\n",
    "In this method, the mask should be different at different moments of the time (otherwise the method simplifies to the dropout by (Gal and Ghahramani)). For x_t you can apply dropout before using LSTM layer.\n",
    "\n",
    "If you have time left, you may implement this method. Choose one of three our implementations as a base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "313px",
    "left": "926px",
    "right": "27px",
    "top": "120px",
    "width": "343px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
