{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch Logo](images/pytorch1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use not only basic functionlaity of `pytorch` but also **`torchvision`** computer vision library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.6.0\r\n",
      "torchvision==0.7.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch as a constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with datasets\n",
    "\n",
    "For data loading pytorch defines **`Dataset`** entity.\n",
    "\n",
    "This abstract class is defined in `torch.utils.data.dataset`:\n",
    "\n",
    "```python\n",
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "```\n",
    "One should inherit `Dataset` and implement `__getitem__` and `__len__` nethods to create a new data source.\n",
    "\n",
    "An example of such ancestor — `torchvision.datasets.ImageFolder`, which allows us to use imagenet-like dataset based on a directory with `./train/{class}` and `./val/{class}` sub-directories structure:\n",
    "\n",
    "```python\n",
    "imagenet = torchvision.datasets.ImageFolder(\"path/to/imagenet_root/\")\n",
    "```\n",
    "\n",
    "Custom example — a dataset loading images with classes defined in some text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# default_loader - default image loading function, uses accimage or PIL\n",
    "from torchvision.datasets.folder import default_loader\n",
    "\n",
    "class TxtList(Dataset):\n",
    "    def __init__(self, path, transform=None, loader=default_loader):\n",
    "        with open(path) as fin:\n",
    "            self.imgs = [s.strip().split() for s in fin.readlines()]\n",
    "\n",
    "        print(f\"=> Found {len(self.imgs)} entries in {path}\")\n",
    "\n",
    "        self.classes = sorted(set([_[1] for _ in self.imgs]))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        target = self.class_to_idx[target]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo /tmp/1.jpg cat > dataset.tsv\n",
    "!echo /tmp/2.jpg cat >> dataset.tsv\n",
    "!echo /tmp/3.jpg dog >> dataset.tsv\n",
    "!echo /tmp/4.jpg cat >> dataset.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/1.jpg cat\r\n",
      "/tmp/2.jpg cat\r\n",
      "/tmp/3.jpg dog\r\n",
      "/tmp/4.jpg cat\r\n"
     ]
    }
   ],
   "source": [
    "!cat dataset.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Found 4 entries in dataset.tsv\n"
     ]
    }
   ],
   "source": [
    "catdog = TxtList(\"dataset.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdog.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/tmp/1.jpg', 'cat'],\n",
       " ['/tmp/2.jpg', 'cat'],\n",
       " ['/tmp/3.jpg', 'dog'],\n",
       " ['/tmp/4.jpg', 'cat']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdog.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(catdog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-09c24881b9b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# FileNotFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcatdog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8afd9d7bb6fb>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/1.jpg'"
     ]
    }
   ],
   "source": [
    "# FileNotFoundError\n",
    "catdog[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchvision` has another useful classes for using standard datasets: \n",
    "http://pytorch.org/docs/master/torchvision/datasets.html.\n",
    "\n",
    "Some of them can be preloaded with built-in functionality, for example **MNIST**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -d \"/tmp/mnist/\" ] && rm -r \"/tmp/mnist/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f571ffd8d58444bdbf6c02dbb05e438b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825020fa8cb24239b0bbb109ad83dbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415d458f3da84b2abc61b114099a08fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd435864e66d4ef7b126e0e3612289d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/mnist/MNIST/raw\n",
      "Processing...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Done!\n",
      "CPU times: user 787 ms, sys: 134 ms, total: 921 ms\n",
      "Wall time: 4.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizhib/.anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "%time mnist = MNIST(\"/tmp/mnist/\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "image, target = mnist[0]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(image), \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Hometask!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement **`UrlList`** dataset which costructor takes list of urls as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class UrlList(Dataset):\n",
    "    def __init__(self, urls: List[str]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate how it works with some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation\n",
    "\n",
    "In the example shown before and in built-in `ImageFolder` `__init__` method has `transform` parameter (and `target_transform`).\n",
    "\n",
    "They are used to transform images/targets loaded into predefined range and form.\n",
    "\n",
    "There is `transforms` sub-module in `torchvision` library which has some examples of such transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example `transforms.ToTensor()` transforms uint8 `PIL` [0, 256)-domained images into [0, 1)-domained tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pil_image = mnist[0][0]\n",
    "th_image = to_tensor(pil_image)\n",
    "th_image.shape, th_image.min(), th_image.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can define `normalize` to implement a standard ImageNet preparation step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transforms.Compose` is used to sequence several compositions as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_and_tensorize = transforms.Compose([\n",
    "    transforms.CenterCrop(16),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(crop_and_tensorize(pil_image)[0].numpy(), \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a custom transformer we should only implement `__call__` method its implementation:\n",
    "\n",
    "```python\n",
    "class HorizontalFlip(object):\n",
    "    def __init__(self, mode=0):\n",
    "        self.method = mode\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if self.method:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "\n",
    "```\n",
    "\n",
    "It is adviced to take a glance at http://pytorch.org/docs/master/torchvision/transforms.html for standard transformations overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional reading\n",
    "\n",
    "Another example of a good set of pre-defined transformations is [`albumentations`](https://github.com/albu/albumentations) library.\n",
    "\n",
    "It supports not only image transformations but it can simultaniously transform its corresponding masks and bboxes.\n",
    "\n",
    "One can also use [`imgaug`](https://github.com/aleju/imgaug) augmentations library but needs to implement `imguag output` -> `tensor` transformations on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Hometask!</font>\n",
    "\n",
    "Implement a transformer that applies random transformation from $D_4$ transformations group.\n",
    "\n",
    "These transformations are very usefull for lossless augmentations in satellite images analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomD4(object):\n",
    "    def __call__(self, img):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate how it works on some MNIST images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders\n",
    "\n",
    "The main reason to implement `Dataset` class is a magic power of `torchvision` loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaders are built on top of some dataset and allow batch iterating over it.\n",
    "\n",
    "Those batches in their turn are created with applying the transformations defined in background processes.\n",
    "\n",
    "Let's look at MNIST dataset with a simple transformer applied as loaders use tensors and not `PIL.Image`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_mnist = MNIST(\"/tmp/mnist/\", train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_loader = DataLoader(transformed_mnist, batch_size=16, shuffle=True, num_workers=4)  # shuffle note here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader standard parameter values are shown above.\n",
    "\n",
    "One can also add **`pin_memory=True`** to page-lock the memory thus making faster cpu-to-cuda transfer with a non-blocking option.\n",
    "\n",
    "**`drop_last=True`** is for avoiding batches size skew in the training process.\n",
    "\n",
    "Let us demonstrate how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:02<00:00, 1376.94it/s]\n"
     ]
    }
   ],
   "source": [
    "for images, targets in tqdm(mnist_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are image batches as tensors on the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loader instance pre-calculate the total number of batches (what makes `tqdm` happy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data example (will be random on each run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANhklEQVR4nO3df6xU9ZnH8c9HpEpso7BGQatrQWPWmOztikRj3bgxbbz+A8S4lujG9ddtFCKocZewf6AxJmZ3u/yl1VtqimvXpgkSTKNpzU2z7gbTiAQVyhZdvLYUBF0MvcTELvLsH/ewucU7Zy5z5swZeN6v5GZmzjNzzpMTPpwz8z0zX0eEAJz8Tmm6AQC9QdiBJAg7kARhB5Ig7EASp/ZyY7b56B+oWUR4suWVjuy2b7D9a9vv2V5ZZV0A6uVOx9ltT5O0U9I3Je2W9IakJRHxq5LXcGQHalbHkX2BpPciYldE/EHSjyUtrLA+ADWqEvbzJf12wuPdxbI/YnvI9mbbmytsC0BFVT6gm+xU4Qun6RExLGlY4jQeaFKVI/tuSRdMePxVSXuqtQOgLlXC/oakS2x/zfaXJH1b0kvdaQtAt3V8Gh8Rh20vk/QzSdMkPRsR27vWGYCu6njoraON8Z4dqF0tF9UAOHEQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BET6ds7me33XZbab3Kr/CuXr26tH7xxRd3vO52nnzyydL60qVLS+t33HFHaf3w4cPH3dNRn376aWl9w4YNHa8bX8SRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYBbXwpEjR0rrvdxPWXz22Wel9ZGRkdq2vXz58tL6rl27att23VrN4lrpohrbo5LGJH0u6XBEzK+yPgD16cYVdH8VER93YT0AasR7diCJqmEPST+3/abtocmeYHvI9mbbmytuC0AFVU/jr4mIPbbPkfSq7f+KiNcmPiEihiUNS/39AR1wsqt0ZI+IPcXtfkkbJC3oRlMAuq/jsNs+w/ZXjt6X9C1J27rVGIDu6nic3fZcjR/NpfG3A/8WEY+3eU3fnsbXOc6+fv360vqHH37Y8bqbNjg4WFqfO3dujzo5Pu+//35pfc2aNaX1tWvXltbbXUNQp66Ps0fELkl/3nFHAHqKoTcgCcIOJEHYgSQIO5AEYQeS4Cuuhauuuqq2dW/fvr20PjY2Vtu263bppZeW1mfOnFnbth9/vHSkVwMDAy1rZ511VqVtz5o1q7R+8ODBSuuvotXQG0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcXactB577LGWtVWrVlVaN+PsAPoWYQeSIOxAEoQdSIKwA0kQdiAJwg4k0Y2JHYFGzJgxo7Q+e/bsHnVyYuDIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM6OE1a776TfeeedPerkxND2yG77Wdv7bW+bsGyW7Vdtv1vc1jcTAICumMpp/A8l3XDMspWSRiLiEkkjxWMAfaxt2CPiNUkHjlm8UNK64v46SYu63BeALuv0Pfu5EbFXkiJir+1zWj3R9pCkoQ63A6BLav+ALiKGJQ1L/OAk0KROh9722Z4jScXt/u61BKAOnYb9JUm3F/dvl7SxO+0AqEvb03jbL0i6TtLZtndLWi3pCUk/sX2XpN9IurnOJpHThRdeWFq/5ZZbOl73pk2bSuvPPPNMaX1sbKzjbTelbdgjYkmL0vVd7gVAjbhcFkiCsANJEHYgCcIOJEHYgSSYshmNefjhh0vrN910U2n9yiuv7Hjb8+bNK62Pjo52vO6mMWUzkBxhB5Ig7EAShB1IgrADSRB2IAnCDiTBT0mjkmnTppXW16xZ07J27733lr72lFPKj0UjIyOl9bVr17asffDBB6WvPRlxZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJPg+O0rNmDGjtP7QQw+V1h999NGOt3348OHS+oIFC0rrb731VsfbPpHxfXYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSILvs5/kzjvvvNL64OBgaf2pp54qrZ96auf/hF5++eXS+nPPPVdazzqO3qm2R3bbz9reb3vbhGWP2P6d7a3F3431tgmgqqmcxv9Q0g2TLF8TEQPFX/l/0QAa1zbsEfGapAM96AVAjap8QLfM9tvFaf7MVk+yPWR7s+3NFbYFoKJOw/49SfMkDUjaK+m7rZ4YEcMRMT8i5ne4LQBd0FHYI2JfRHweEUckfV9S+dePADSuo7DbnjPh4WJJ21o9F0B/aDtIavsFSddJOtv2bkmrJV1ne0BSSBqV9J0ae0xv+vTppfXly5e3rC1ZsqT0tQMDAx31dNQnn3xSWn/llVda1u6///5K68bxaRv2iJjsX8sPaugFQI24XBZIgrADSRB2IAnCDiRB2IEk+IprD5x55pml9Xvuuae03u5rqmVDb1Xt3LmztH7zzTeX1rdt4xKMfsGRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9iqZNm9aytmzZstLX3n333aX1yy67rKOepqLdOPnTTz9dWt+4cWNpfXR09HhbQkM4sgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoyzF9r9pPKtt97asvbggw92u53jUjb1cbufkj506FC320Gf4sgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzl5YtGhRab3psfQyV199dcvatddeW/ragwcPdrudE8KWLVtK6+1+q3/27NmVtr9p06ZKr+9E2yO77Qts/8L2DtvbbS8vls+y/artd4vbmfW3C6BTUzmNPyzpoYj4M0lXSVpq+zJJKyWNRMQlkkaKxwD6VNuwR8TeiNhS3B+TtEPS+ZIWSlpXPG2dpPLzYACNOq737LYvkvR1Sb+UdG5E7JXG/0OwfU6L1wxJGqrWJoCqphx221+WtF7Sioj4ve0pvS4ihiUNF+uITpoEUN2Uht5sT9d40H8UES8Wi/fZnlPU50jaX0+LALrBEeUHW48fwtdJOhARKyYs/ydJ/xMRT9heKWlWRPxdm3X17ZG93X44cuRIjzpBLzz//POl9XZfeb788ssrbb/sp8mriohJT7unchp/jaS/kfSO7a3FslWSnpD0E9t3SfqNpPKJugE0qm3YI+I/JbV6g359d9sBUBculwWSIOxAEoQdSIKwA0kQdiCJtuPsXd1YH4+zDw4Oltbvu+++HnXSXddfXz5gctppp/Wok1y2bt1aWr/iiitq23arcXaO7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsJ7nFixeX1k8//fRK61+xYkVpff78+ZXWX5cHHnigtP7RRx9VWv/rr79eWh8dHa20/jKMswPJEXYgCcIOJEHYgSQIO5AEYQeSIOxAEoyzAycZxtmB5Ag7kARhB5Ig7EAShB1IgrADSRB2IIm2Ybd9ge1f2N5he7vt5cXyR2z/zvbW4u/G+tsF0Km2F9XYniNpTkRssf0VSW9KWiTpryUdioh/nvLGuKgGqF2ri2qmMj/7Xkl7i/tjtndIOr+77QGo23G9Z7d9kaSvS/plsWiZ7bdtP2t7ZovXDNnebHtzpU4BVDLla+Ntf1nSv0t6PCJetH2upI8lhaTHNH6qf2ebdXAaD9Ss1Wn8lMJue7qkn0r6WUT8yyT1iyT9NCIub7Mewg7UrOMvwti2pB9I2jEx6MUHd0ctlrStapMA6jOVT+O/Iek/JL0j6UixeJWkJZIGNH4aPyrpO8WHeWXr4sgO1KzSaXy3EHagfnyfHUiOsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kETbH5zsso8lfTDh8dnFsn7Ur731a18SvXWqm739aatCT7/P/oWN25sjYn5jDZTo1976tS+J3jrVq944jQeSIOxAEk2Hfbjh7Zfp1976tS+J3jrVk94afc8OoHeaPrID6BHCDiTRSNht32D717bfs72yiR5asT1q+51iGupG56cr5tDbb3vbhGWzbL9q+93idtI59hrqrS+m8S6ZZrzRfdf09Oc9f89ue5qknZK+KWm3pDckLYmIX/W0kRZsj0qaHxGNX4Bh+y8lHZL03NGptWz/o6QDEfFE8R/lzIj4+z7p7REd5zTeNfXWaprxv1WD+66b0593ookj+wJJ70XEroj4g6QfS1rYQB99LyJek3TgmMULJa0r7q/T+D+WnmvRW1+IiL0RsaW4Pybp6DTjje67kr56oomwny/ptxMe71Z/zfcekn5u+03bQ003M4lzj06zVdye03A/x2o7jXcvHTPNeN/su06mP6+qibBPNjVNP43/XRMRfyFpUNLS4nQVU/M9SfM0PgfgXknfbbKZYprx9ZJWRMTvm+xlokn66sl+ayLsuyVdMOHxVyXtaaCPSUXEnuJ2v6QNGn/b0U/2HZ1Bt7jd33A//y8i9kXE5xFxRNL31eC+K6YZXy/pRxHxYrG48X03WV+92m9NhP0NSZfY/prtL0n6tqSXGujjC2yfUXxwIttnSPqW+m8q6pck3V7cv13SxgZ7+SP9Mo13q2nG1fC+a3z684jo+Z+kGzX+ifx/S/qHJnpo0ddcSW8Vf9ub7k3SCxo/rftfjZ8R3SXpTySNSHq3uJ3VR739q8an9n5b48Ga01Bv39D4W8O3JW0t/m5set+V9NWT/cblskASXEEHJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0n8H71wRoHHg3BKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0][0].numpy(), \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building NN models\n",
    "\n",
    "We should inherit `torch.nn.Module` class to implement pytorch model, `torch.nn` has lots of pre-defined \"building bricks\".\n",
    "\n",
    "Their functional analogs are collected in `torch.nn.functional`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have both module and functional versions of max-pooling, activations, upsempling and some other ops:\n",
    "* `nn.MaxPool2d` / `F.max_pool2d`\n",
    "* `nn.ReLU` / `F.relu`\n",
    "* `nn.Upsample(mode='bilinar')` / `F.upsample(mode='bilinar')` — **deprecated**, use `F.interpolate` instead\n",
    "\n",
    "To compose several nn modules we can use `nn.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    nn.Conv2d(3, 64, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, padding=1),\n",
    "    nn.ReLU()\n",
    "]\n",
    "unet_down1 = nn.Sequential(*layers)\n",
    "print(unet_down1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training\n",
    "\n",
    "To train a model one must define a loss function, some of them can also be found in `torch.nn`:\n",
    "\n",
    "```python\n",
    "output = model(torch.cat(x, 1))\n",
    "target = torch.arange(1, 1001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update the weights manually:\n",
    "\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use built-in optimizer from `torch.optim` family:\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to zero the gradients to disable gradient accumulations that happens by defult (`pytorch` sends its regards to `tf` rnn implementations):\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After zeroing the gradients we can run both forward and backward stage with loss calculation in-between:\n",
    "```python\n",
    "output = net(x)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the weights we can use the optimizer again:\n",
    "```python\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adjust learning rate in some pre-defined way we can use **`torch.optim.lr_scheduler`** sub-module:\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train ResNets on ImageNet with a standard training scheme we can use\n",
    "```python\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "for epoch in range(100):\n",
    "    # no scheduler.step() here* \n",
    "    # train(...)\n",
    "    # validate(...)\n",
    "    scheduler.step()  # == scheduler.step(epoch)\n",
    "```\n",
    "\\* one **should not** put `scheduler.step()` before `optimizer.step()`, otherwise the first epoch will be \"skipped\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several libraries out there like [`pytorch-lightning`](https://github.com/PyTorchLightning/pytorch-lightning), [`kekas`](https://github.com/belskikh/kekas), [`catalyst`](https://github.com/catalyst-team/catalyst) and others that make building pipelines even easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch in the wild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation model creation (with U-Net as an example)\n",
    "\n",
    "![U-Net scheme](images/unet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-defined heavily-used convolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, dilation=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One **encoder block** consists of two sequential convolutions, an activation layer and an optional batch-norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels)\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        if self.batch_norm:\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = EncoderBlock(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential()\n",
    "        self.block.add_module(\"conv1\", conv3x3(in_channels, out_channels))\n",
    "        if batch_norm:\n",
    "            self.block.add_module(\"bn1\", nn.BatchNorm2d(out_channels))\n",
    "        self.block.add_module(\"relu1\", nn.ReLU())\n",
    "        self.block.add_module(\"conv2\", conv3x3(out_channels, out_channels))\n",
    "        if batch_norm:\n",
    "            self.block.add_module(\"bn2\", nn.BatchNorm2d(out_channels))\n",
    "        self.block.add_module(\"relu2\", nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (block): Sequential(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = EncoderBlock(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And its \"functional\" version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(in_channels, out_channels, batch_norm=False):\n",
    "    block = nn.Sequential()\n",
    "    block.add_module(\"conv1\", conv3x3(in_channels, out_channels))\n",
    "    if batch_norm:\n",
    "        block.add_module(\"bn1\", nn.BatchNorm2d(out_channels))\n",
    "    block.add_module(\"relu1\", nn.ReLU())\n",
    "    block.add_module(\"conv2\", conv3x3(out_channels, out_channels))\n",
    "    if batch_norm:\n",
    "        block.add_module(\"bn2\", nn.BatchNorm2d(out_channels))\n",
    "    block.add_module(\"relu2\", nn.ReLU())\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = encoder_block(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder is composed from several encoder blocks.\n",
    "\n",
    "Its final form is defined bu the number of input channels, the number of channels in the first block output and the number of such blocks.\n",
    "\n",
    "And we need to store preliminary activatons to apply Decoder on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        for i in range(num_blocks):\n",
    "            in_channels = in_channels if not i else num_filters * 2 ** (i - 1)\n",
    "            out_channels = num_filters * 2 ** i\n",
    "            self.add_module(f\"block{i + 1}\", encoder_block(in_channels, out_channels))\n",
    "            if i != num_blocks - 1:\n",
    "                self.add_module(f\"pool{i + 1}\", nn.MaxPool2d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.__getattr__(f\"block{i + 1}\")(x)\n",
    "            acts.append(x)\n",
    "            if i != self.num_blocks - 1:\n",
    "                x = self.__getattr__(f\"pool{i + 1}\")(x)\n",
    "        return acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can use **`add_module`** way of layers definition as its number is variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (block1): Sequential(\n",
       "    (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2): Sequential(\n",
       "    (conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3): Sequential(\n",
       "    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4): Sequential(\n",
       "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(in_channels=3, num_filters=8, num_blocks=4)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 8, 512, 512]),\n",
       " torch.Size([4, 16, 256, 256]),\n",
       " torch.Size([4, 32, 128, 128]),\n",
       " torch.Size([4, 64, 64, 64])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(4, 3, 512, 512)\n",
    "\n",
    "[_.shape for _ in encoder(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder block consists of upscaling \"lower\" output and concatenating it with a saved encoder block output from the \"left\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Upsample = nn.Upsample\n",
    "\n",
    "# class Upsample(nn.Module):\n",
    "#     def __init__(self, scale_factor=2, mode='bilinear'):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.scale_factor = scale_factor\n",
    "#         self.mode = mode\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=True)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.uppool = Upsample(scale_factor=2, mode=\"bilinear\")\n",
    "        self.upconv = conv3x3(out_channels * 2, out_channels)\n",
    "        self.conv1 = conv3x3(out_channels * 2, out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, down, left):\n",
    "        x = self.uppool(down)\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([left, x], 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = DecoderBlock(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 16, 256, 256]), torch.Size([4, 8, 512, 512]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1].shape, y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizhib/.anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 512, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block(y[1], y[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build Decoder from several decoder blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.add_module(f\"block{num_blocks - i}\", DecoderBlock(num_filters * 2 ** i))\n",
    "\n",
    "    def forward(self, acts):\n",
    "        up = acts[-1]\n",
    "        for i, left in enumerate(acts[-2::-1]):\n",
    "            up = self.__getattr__(f\"block{i + 1}\")(up, left)\n",
    "        return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 512, 512])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 8, 512, 512]),\n",
       " torch.Size([4, 16, 256, 256]),\n",
       " torch.Size([4, 32, 128, 128]),\n",
       " torch.Size([4, 64, 64, 64])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in encoder(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 512, 512])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(encoder(x)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net is build from Encoder, Decoder and a final classification layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, num_filters=64, num_blocks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        print(f\"=> Building {num_blocks}-blocks {num_filters}-filter U-Net\")\n",
    "\n",
    "        self.encoder = Encoder(in_channels, num_filters, num_blocks)\n",
    "        self.decoder = Decoder(num_filters, num_blocks - 1)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = self.encoder(x)\n",
    "        x = self.decoder(acts)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Integration testing\" (pytorch 0.3 legacy code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building 4-blocks 64-filter U-Net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizhib/.anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 416, 416])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "model = UNet(num_classes=1)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "images = Variable(torch.randn(4, 3, 416, 416), volatile=True)\n",
    "if torch.cuda.is_available():\n",
    "    images = images.cuda()\n",
    "\n",
    "model.forward(images).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "/home/nizhib/.anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
    "  import sys\n",
    "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
    "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
    "/home/nizhib/.anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
    "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for **pytorch 0.4+**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building 4-blocks 64-filter U-Net\n",
      "torch.Size([4, 1, 416, 416])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet(num_classes=1)\n",
    "model.to(device)\n",
    "\n",
    "images = torch.randn(4, 3, 416, 416).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(model.forward(images).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get linear activations as an output.\n",
    "\n",
    "To train such models we need to use **/.\\*WithLogits/** loss functions subset\n",
    "\n",
    "We can use `torch.sigmoid` or `torch.softmax` to get probabilities (**0.4.1+**, `torch.nn.functional.sigmoid/softmax` before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder blocks structure seen before looks exactly like VGG architecture:\n",
    "\n",
    "![img](https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png)\n",
    "\n",
    "Let us have a look at VGG model from `torchvision` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG13 is the VGG version with 2 convolutional layer in each block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg13()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): ReLU(inplace=True)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the classifier, only the features are useful.\n",
    "\n",
    "They are built from conv-relu-conv-relu + maxpooling.\n",
    "\n",
    "Let's build encoder blocks via grouping VGG layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG13Encoder(nn.Module):\n",
    "    def __init__(self, num_blocks, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        backbone = vgg13(pretrained=pretrained).features\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            block = nn.Sequential(*[backbone[j] for j in range(i * 5, i * 5 + 4)])\n",
    "            self.add_module(f\"block{i + 1}\", block)\n",
    "            if i != num_blocks - 1:\n",
    "                self.add_module(f\"pool{i + 1}\", nn.MaxPool2d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.__getattr__(f\"block{i + 1}\")(x)\n",
    "            acts.append(x)\n",
    "            if i != self.num_blocks - 1:\n",
    "                x = self.__getattr__(f\"pool{i + 1}\")(x)\n",
    "        return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg13-c768596a.pth\" to /home/nizhib/.cache/torch/hub/checkpoints/vgg13-c768596a.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b18ab8a34c4a60badb9ac906cd8631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=532194478.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG13Encoder(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_encoder = VGG13Encoder(num_blocks=4, pretrained=True)\n",
    "vgg_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare it to \"vanilla\" encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (block1): Sequential(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2): Sequential(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3): Sequential(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4): Sequential(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(in_channels=3, num_filters=64, num_blocks=4)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both structures are identical!\n",
    "\n",
    "But now we have some usefull pretrained weights in the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Hometask!</font>\n",
    "\n",
    "### Part1. Toy dataset\n",
    "\n",
    "\n",
    "**Implement** toy dataset to generate noisy ellipses like that:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/jakeret/tf_unet/master/docs/toy_problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should output both the image and its corresponding mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ellipses(Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def moy_variant(surname):\n",
    "    return int(hashlib.md5(surname.encode().lower()).hexdigest()[-1], 16) % 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement FANet** (https://arxiv.org/abs/2007.03815, if `moy_variant` returns 1 for you) **or BiCANet** (https://arxiv.org/abs/2003.09669, otherwise) based on pretrained **DenseNet** (pytorch) **and DPN** (cadene/pretrainedmodels) networks instead of ResNets from the papers.\n",
    "\n",
    "It is advised to use `BCEWithLogitsLoss` as a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FADenseNet(nn.Module):\n",
    "    def __init__(self, num_classes, backbone=\"densenet161\"):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FADPN(nn.Module):\n",
    "    def __init__(self, num_classes, arch=\"dpn92\"):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BiCADenseNet(nn.Module):\n",
    "    def __init__(self, num_classes, arch=\"densenet161\"):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BiCADPN(nn.Module):\n",
    "    def __init__(self, num_classes, arch=\"dpn92\"):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstrate** how they are trained with `tensorboard` screenshots (use torchvision built-in tools).\n",
    "\n",
    "That includes but not limited to loss curves, masks from different epochs etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Portrait Segmentation\n",
    "\n",
    "Repeat the training procedures on some real task: http://xiaoyongshen.me/webpage_portrait/.\n",
    "\n",
    "Easy-to-use dataset version can be downloaded from https://yadi.sk/d/1SSkfLh4WnEhmw.\n",
    "\n",
    "Use `dice score` as a target scoring function, it should be published in the final report.\n",
    "\n",
    "All the loss charts, mask quality evolution and final mask examples are expected as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Bells and whistles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use at least 5 `albumentations` or `imgaug` augmentations on the data preparation step (up to 0.5 bonus points);\n",
    "\n",
    "\n",
    "* Use `pytorch-lightning`, `kekas` or `catalyst` to build the training pipeline (up to 0.5 bonus points);\n",
    "\n",
    "\n",
    "* Deploy the web demo to play with on your own `dl2020-{lastname}.ml` domain.\n",
    "\n",
    "  The demo earns 0.1 bonus points for each day it's alive starting from **Oct 12 00:00 till Oct 25 23:59**.\n",
    "\n",
    "  You can use https://github.com/nizhib/portrait-demo as a start point (up to 2 bonus points for 2-week streak)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#cc6666'>Part 4. MMP specials</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Integrate fp16 mixed precision in your training pipeline ([amp docs](https://pytorch.org/docs/stable/notes/amp_examples.html));\n",
    "\n",
    "\n",
    "* Having `dl2020-{lastname}.ml` demo deployed starting from **Oct 26 00:00 till Nov 01 23:59** is a must."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
